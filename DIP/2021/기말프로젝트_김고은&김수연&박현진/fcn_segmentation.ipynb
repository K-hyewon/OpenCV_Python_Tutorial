{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead58acb-67e6-4291-92a6-1f090f731e5d",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac44006-3177-4888-8557-59722c17116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Basic module\n",
    "\n",
    "import os # Import path\n",
    "import cv2 # Imread Image\n",
    "import glob # Import path\n",
    "import random # Create random variable\n",
    "import os.path # Import path\n",
    "\n",
    "import numpy as np # For high-performance numerical calculation.\n",
    "import matplotlib.pyplot as plt # makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels\n",
    "\n",
    "## 2. image preprocess\n",
    "\n",
    "from random import randrange # Returns a randomly selected element from the specified range.\n",
    "from patchify import patchify # Split image into small, overlappable patches, and merge patches back into the original image.\n",
    "import imgaug.augmenters as iaa # Helps to easily apply various Data Augmentation techniques\n",
    "\n",
    "## 3. json \n",
    "\n",
    "import json # JSON (JavaScript Object Notation), An open standard format using human-readable text to deliver a data object consisting of a key-value pair\"\n",
    "from collections import OrderedDict # Guarantees the order of json data\n",
    "\n",
    "## 4. model \n",
    "\n",
    "import timeit # Measure execution time of small code snippets\n",
    "import seaborn as sns #  Make statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures.\n",
    "import tensorflow as tf # Provides a variety of functions to easily implement deep learning programs created by Google for model training\n",
    "from sklearn.model_selection import train_test_split # train and test split function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b0536-a050-425d-ae14-5e0b273e8131",
   "metadata": {},
   "source": [
    "# Make folder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb96851-bc54-4038-9276-d25cf7353142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkfolder(folder):\n",
    "\n",
    "    for j in range(len(folder)):\n",
    "        if not os.path.exists(folder[j]):\n",
    "            os.makedirs(folder[j])\n",
    "  \n",
    "'''\n",
    " mkfolder(folder) 함수 설명 \n",
    " \n",
    "    *data_path, train_path는 string 타입의 path\n",
    "    *mkfolder안에 입력할 값 = list 타입\n",
    "\n",
    "    ex_\n",
    "\n",
    "    folder = [data_path,train_path]\n",
    "\n",
    "    mkfolder(folder)\n",
    "    -> data_path,train_path에 대한 폴더가 생성된다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500475c5-b6d8-4805-93fd-e2c70fbcfc8d",
   "metadata": {},
   "source": [
    "# Make Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dcca7-71d9-49a4-909b-e5085fba2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기까지는 한 번 확인하기 \n",
    "base_path = './DIP/project/'\n",
    "ori_path='./DIP/data/18-40-02-02 (SLA).jpg' #원본 이미지\n",
    "lbl_img_path='./DIP/labelled_data/18-40-02-02 (SLA)_re_image.png' #원본 labelled 이미지\n",
    "\n",
    "fnr_path='./DIP/data/aug/remove_lt.jpg' #글씨 제거 이미지 \n",
    "\n",
    "#Data Path\n",
    "DIP_path=os.path.join(base_path,'DIP')\n",
    "data_path = os.path.join(DIP_path, 'data')\n",
    "lbl_path = os.path.join(DIP_path, 'labelled_data')\n",
    "\n",
    "# 원본이미지 어그멘테이션 폴더 \n",
    "data_aug_path = os.path.join(data_path,'aug')\n",
    "aug_ver1_path = os.path.join(data_aug_path,'ver_1')\n",
    "aug_ver2_path = os.path.join(data_aug_path,'ver_2')\n",
    "aug_patch_path = os.path.join(aug_ver1_path,'patches')\n",
    "aug2_patch_path = os.path.join(aug_ver2_path,'patches')\n",
    "\n",
    "# 라벨이미지 어그멘테이션 폴더 \n",
    "lbl_aug_path = os.path.join(lbl_path,'aug')\n",
    "lbl_aug_ver1_path = os.path.join(lbl_aug_path,'ver_1')\n",
    "lbl_aug_ver2_path = os.path.join(lbl_aug_path,'ver_2')\n",
    "lbl_aug_patch_path = os.path.join(lbl_aug_ver1_path,'patches')\n",
    "lbl_aug2_patch_path = os.path.join(lbl_aug_ver2_path,'patches')\n",
    "\n",
    "# json 폴더 \n",
    "json_path_ver1 = os.path.join(aug_ver1_path,'json')\n",
    "json_path_ver2= os.path.join(aug_ver2_path,'json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecac7a2-777e-424c-85c6-7c865846867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_list = [DIP_path,data_path,lbl_path,\n",
    "             data_aug_path,aug_ver1_path,aug_ver2_path,aug_patch_path,aug2_patch_path,\n",
    "            lbl_aug_path,lbl_aug_ver1_path,lbl_aug_ver2_path,lbl_aug_patch_path,lbl_aug2_patch_path ,\n",
    "             json_path_ver1, json_path_ver2]\n",
    "\n",
    "mkfolder(fold_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2caae-1956-4a95-b17e-9f8065021f67",
   "metadata": {},
   "source": [
    "## 밑에 글씨 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c023d7-6620-48fd-8caa-4e752201e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wr(ori_path,save_path):\n",
    "    original = cv2.imread(ori_path, cv2.IMREAD_COLOR)\n",
    "    kernel=np.ones((11,11),np.uint8)\n",
    "    ori_opened = cv2.morphologyEx(original[1100:1200,1300:1600,:], cv2.MORPH_OPEN, kernel) #글씨 영역 선택 \n",
    "    original[1100:1200,1300:1600,:] = ori_opened #morphological 연산 중 opening 이용 \n",
    "    cv2.imwrite(save_path+'/remove_lt.jpg', original) #새로운 폴더에 글씨 제거한 이미지 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b1fe2-9b85-4619-9601-579e972c85eb",
   "metadata": {},
   "source": [
    "## Labelled Image Red to Black "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee11e7-f205-449b-884c-7d19b62aa79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RedtoBlack(lbl_img_path, lbl_path):    \n",
    "    img=cv2.imread(lbl_img_path)\n",
    "    height, width, _ = img.shape \n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            # img[i,j] is the RGB pixel at position (i, j)\n",
    "            # check if it's [0, 0, 0] and replace with [255, 255, 255] if so\n",
    "            if (img[i,j])[2] >= 250:\n",
    "                img[i, j] = [0, 0, 0]\n",
    "    cv2.imwrite(lbl_path+'/black_lbl_img.jpg',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75af53-3bf8-4c02-8006-1a76d26e1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RedtoBlack(lbl_img_path,lbl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53438740-a09a-484e-99f9-e571a74ba27c",
   "metadata": {},
   "source": [
    "## Augmentation Ver.1 \n",
    "### Rotation & Flip & Translation & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88148392-4211-4b10-8fd8-5471d80316d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. rotation\n",
    "# 2. flip\n",
    "#aug1_path = os.path.join(aug_path,'aug_ver1') # aug한 이미지 저장할 폴더 \n",
    "\n",
    "'''\n",
    "aug_image function:\n",
    "[arguments]\n",
    "- path = augmentation적용할 이미지를 불러올 폴더\n",
    "- deg = float 또는 int type의 rotation시킬 각도(random변수로 앞에서 지정)\n",
    "\n",
    "[function]\n",
    "M = cv2.getRotationMatrix2D((x,y),angle,scale)\n",
    "(x,y) = 회전 중심점\n",
    "angle = 회전 중심점을 기준으로 회전할 각도\n",
    "scale = 이미지의 확대 및 축소 비율 (default:1 = 인풋 크기 그대로)\n",
    "cv2.warpAffine(image, M, (dsize)) = 아핀 변환 함수(cv2.warpAffine)로 회전 변환을 계산\n",
    "-> M = 계산할 matrix\n",
    "-> dsize = tuple 타입의 출력 이미지의 너비와 높이.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812608e-dd92-43be-8487-e747bf0ba5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extension= '.jpg'\n",
    "\n",
    "#aug1_path = image \n",
    "#aug2_path = lbl_image \n",
    "\n",
    "def aug_image(fnr_path, lbl_fnr_path, aug1_path, aug2_path):\n",
    "    global flip_dic #json 파일 생성을 위해 global 함수로 선언해줌\n",
    "    image = cv2.imread(fnr_path,cv2.IMREAD_COLOR) #글씨 제거한 사진 input으로 넣어줌\n",
    "    lbl_image= cv2.imread(lbl_fnr_path,cv2.IMREAD_COLOR) #labelled_data는 원본으로 넣어줌 \n",
    "    rows,cols,c = image.shape\n",
    "\n",
    "    \n",
    "    print(\"This time, We are going to 'Rotate' and 'Flip' your image.\\n\")\n",
    "    print('*------------------------------------------------------------*\\n')\n",
    "    print(\"First, Input the degree! It wil be the maximum value on your 'countclockwise' rotation range.\")\n",
    "    x = int(input('minimum of degree:'))\n",
    "    y = int(input('maximum of degree:'))\n",
    "    print(\"\\n\")\n",
    "    print(\"Let's move on to 'FLIP'\\n\")\n",
    "    print(\"We'll give you THREE options. Please choose one of them.\\n\")\n",
    "    print(\"OPTION 1: Flip the image vertically, horizontally.\")\n",
    "    print(\"OPTION 2: Flip the image ONLY vertically.\")\n",
    "    print(\"OPTION 3: Flip the image ONLY horizontally.\\n\")\n",
    "    ans_flip = int(input(\"What option would you like to choose?(Just enter the option number):\"))\n",
    "    \n",
    "    # Rotation\n",
    "    deg = randint(x,y) #회전할 각도를 x, y 사이의 random한 정수로 받음 \n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), deg, 1) #회전중심, 회전 중심점 기준 회전할 각도, 이미지 확대/축소 비율\n",
    "    aug = cv2.warpAffine(image, M, (cols, rows))\n",
    "    aug2 = cv2.warpAffine(lbl_image, M, (cols, rows))\n",
    "    \n",
    "    # Flip\n",
    "    if ans_flip == 1: \n",
    "        aug = cv2.flip(aug,1) # horizontal\n",
    "        aug = cv2.flip(aug,0) # vertical \n",
    "        aug2 = cv2.flip(aug2,1) # horizontal\n",
    "        aug2 = cv2.flip(aug2,0) # vertical \n",
    "        cv2.imwrite(aug1_path + \"/rot-%s-flipVH\"%str(deg)+Extension, aug)\n",
    "        cv2.imwrite(aug2_path + \"/rot-%s-flipVH\"%str(deg)+Extension, aug2)\n",
    "        v, h ='yes','yes'\n",
    "    elif ans_flip == 2: #vertical flip only\n",
    "        aug = cv2.flip(aug,0)\n",
    "        aug2 = cv2.flip(aug2,0) \n",
    "        cv2.imwrite(aug1_path + \"/rot-%s-flipV\"%str(deg)+Extension, aug)\n",
    "        cv2.imwrite(aug2_path + \"/rot-%s-flipV\"%str(deg)+Extension, aug2)\n",
    "        v, h = 'yes','no'\n",
    "    elif ans_flip == 3: #horizontal flip only\n",
    "        aug = cv2.flip(aug,1)\n",
    "        aug2 = cv2.flip(aug2,1)\n",
    "        cv2.imwrite(aug1_path + \"/rot-%s-flipH\"%str(deg)+Extension, aug)\n",
    "        cv2.imwrite(aug2_path + \"/rot-%s-flipH\"%str(deg)+Extension, aug2)\n",
    "        v, h = 'no','yes'\n",
    "\n",
    "    else:\n",
    "        print('[WARNING : TRY AGAIN]')\n",
    "    \n",
    "    flip_dic = {'rotation range (degree)':[x,y],'rotation degree':deg,'vertical flip':v,'horizental flip': h}\n",
    "    return flip_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6747bbd-c617-4174-a379-b9193e5bbdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_img_path=lbl_path+'/black_lbl_img.jpg'\n",
    "aug_image(fnr_path, lbl_img_path, aug_ver1_path, lbl_aug_ver1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6041e5-cbb2-4481-960c-78e82ca7ac14",
   "metadata": {},
   "source": [
    "### PadToFixedSize (Padding + Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787df76-2f59-4642-9f33-d9a26e21307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이후에 128 x 128 사이즈로 patch를 잘라주기 때문에 잘라주면서 남는 공간이 생기지 않도록 128의 배수를 가진 사이즈로 padding 시켜줌 \n",
    "\n",
    "def padding(img,lbl_img, aug1_path, aug2_path, patch_size):\n",
    "    global a, b\n",
    "    x=img.shape[1] #사진의 width = column\n",
    "    y=img.shape[0] #사진의 height = row\n",
    "\n",
    "    maa=patch_size*(max(x,y)//patch_size+2) #128의 배수로 만들기 위해 처리해줌 \n",
    "    mii=patch_size*(max(x,y)//patch_size+1)\n",
    "\n",
    "    if min == x:\n",
    "        a = maa\n",
    "        b = mii\n",
    "        print('width')\n",
    "    else:\n",
    "        a = mii\n",
    "        b = maa\n",
    "        print('height')\n",
    "\n",
    "    a = randrange(mii, maa, patch_size) #mii, maa 사이에서 step=128 (128의 배수)인 random한 정수\n",
    "    b = randrange(mii, maa, patch_size) \n",
    "\n",
    "    aug=iaa.PadToFixedSize(width=b, height=a, position='left-top') #padding을 수행하여 이미지 사이즈로 만들어줌 + translation\n",
    "    pad=aug(image=img)\n",
    "    pad_label=aug(image=lbl_img)\n",
    "\n",
    "    cv2.imwrite(aug1_path + '/af_pad.jpg',pad)\n",
    "    cv2.imwrite(aug2_path +'/af_pad.jpg',pad_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b00895-6460-430a-b818-0eb8149a0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=128   \n",
    "\n",
    "img_path=glob.glob(aug_ver1_path+'/rot*.jpg') #glob 함수를 이용하여 폴더 내 rot로 시작하는 이름을 가진 flip&rotate마친 후 파일 불러들임 \n",
    "img=cv2.imread(img_path[0])\n",
    "\n",
    "lbl_rot_path = glob.glob(lbl_aug_ver1_path+'/rot*.jpg')\n",
    "lbl_img= cv2.imread(lbl_rot_path[0]) \n",
    "\n",
    "padding(img,lbl_img, aug_ver1_path, lbl_aug_ver1_path, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e92780-6346-44e4-ab38-86c894cffe10",
   "metadata": {},
   "source": [
    "## Patch 자르고 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed552e6d-c9cd-4cfc-9fe8-9424478fd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_path = aug_ver1_path+'/af_pad.jpg'\n",
    "lbl_pad_path=lbl_aug_ver1_path+'/af_pad.jpg'\n",
    "\n",
    "save_patches(pad_path,aug_patch_path,patch_size)\n",
    "save_patches(lbl_pad_path,lbl_aug_patch_path,patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d57dc-6ac5-4f03-8657-f3bc2b1fa316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkJson(JSON):\n",
    "    file_data = OrderedDict()\n",
    "\n",
    "    file_data['Translation(padding)'] = {'paddingBox_height':JSON[0],'paddingBox_width':JSON[1]}\n",
    "    file_data['Rotation_counterClockwise'] = {'range(dgree)':JSON[2],'degree':JSON[3]}\n",
    "    file_data['Flip'] = {'vertical':JSON[4], 'horizontal':JSON[5]}\n",
    "    file_data['number of patches'] = JSON[6]\n",
    "    file_data['size of patches'] = JSON[7]\n",
    "    \n",
    "    file_data['contrast alpha'] = JSON[8]\n",
    "    file_data['shear'] = JSON[9]\n",
    "    \n",
    "    print(json.dumps(file_data,ensure_ascii=False,indent='\\t'))\n",
    "    if JSON[8] == None:\n",
    "        with open(json_path_ver1 + '/descriptor.json','w',encoding='utf-8') as make_file:\n",
    "            json.dump(file_data,make_file,ensure_ascii=False,indent='\\t')\n",
    "    else:\n",
    "        with open(json_path_ver2 + '/descriptor.json','w',encoding='utf-8') as make_file:\n",
    "            json.dump(file_data,make_file,ensure_ascii=False,indent='\\t')\n",
    "            \n",
    "# JSON = [a,b,flip_dict['rotation range (degree)'],flip_dict['rotation degree'],flip_dict['vertical flip'],flip_dict['horizental flip'],p_num]\n",
    "# mkJson(JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c67ad-acaa-479a-9cea-c33ae8f6ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON = [a,b,\n",
    "        flip_dic['rotation range (degree)'],\n",
    "        flip_dic['rotation degree'],\n",
    "        flip_dic['vertical flip'],flip_dic['horizental flip'],\n",
    "        p_num,patch_size,None,None]\n",
    "\n",
    "mkJson(JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc6d43-35d9-450b-be1f-ed14b4c20186",
   "metadata": {},
   "source": [
    "## Augmentation Ver.2 \n",
    "### Shear + GrayScale + Contrast + Flip + Rotation + Translation + Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fadeda-3f5b-487d-bcd2-5f596f5233cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear(fnr_path, lbl_img_path):\n",
    "    global shear\n",
    "    src = cv2.imread(fnr_path) \n",
    "    lbl_src=cv2.imread(lbl_path+'/black_lbl_img.jpg')\n",
    "\n",
    "    shear=uniform(0,2)\n",
    "    aff = np.array([[1, 0.5, 0], [0, 1, 0]], dtype=np.float32)\n",
    "\n",
    "    h, w = src.shape[:2] \n",
    "    dst=cv2.warpAffine(src, aff, (w + int(h * shear), h)) #x축을  y축 대비 shear의 이븅로 기울인 효과 \n",
    "    lbl_dst=cv2.warpAffine(lbl_src, aff, (w + int(h * shear), h))\n",
    "    cv2.imwrite(aug_ver2_path+'/shear.jpg',dst)\n",
    "    cv2.imwrite(lbl_aug_ver2_path+'/shear.jpg',lbl_dst)\n",
    "    return shear\n",
    "\n",
    "#dst의 크기는 affine 변환 행렬에서 x축 방향으로 늘어난 만큼 더 더해주어야 합니다.\n",
    "# affine 변환 행렬에서 x축의 사이즈가 늘어난 크기는 y축 사이즈의 shear배 만큼 늘어나게 되므로 (h*shear)를 w에 더해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f80bd-8e07-4d21-b2ad-6ed3805a00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_img_path=lbl_path+'/black_lbl_img.jpg'\n",
    "shear(fnr_path,lbl_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f535d02a-d79f-469a-bda3-8464386fc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545563b-ff5b-40d7-9614-d98bd84a171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnr_path=glob.glob(aug_ver2_path+'/contrast*.jpg') #contrast 진행 후 이미지를 glob함수로 불러들여서 flip & rotate 진행 \n",
    "fnr_path=fnr_path[0]\n",
    "\n",
    "lbl_fnr_path=glob.glob(lbl_aug_ver2_path+'/contrast*.jpg')\n",
    "lbl_fnr_path=lbl_fnr_path[0]\n",
    "\n",
    "\n",
    "aug_image(fnr_path, lbl_fnr_path, aug_ver2_path, lbl_aug_ver2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afb365-d61e-4fa0-8376-4cd82f8cf669",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=glob.glob(aug_ver2_path+'/rot*.jpg')\n",
    "img=cv2.imread(img_path[0])\n",
    "\n",
    "lbl_path = glob.glob(lbl_aug_ver2_path+'/rot*.jpg')\n",
    "lbl_img= cv2.imread(lbl_path[0])\n",
    "\n",
    "padding(img,lbl_img, aug_ver2_path, lbl_aug_ver2_path, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcf73a-29b6-4a2b-a7d9-757fcd93f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch_size=128\n",
    "\n",
    "def save_patches_ver2(pad_path,save_path,patch_size):\n",
    "    global p_num\n",
    "    img = cv2.imread(pad_path) #padding 후의 이미지 input으로 넣어주기\n",
    "    # one image -> patches\n",
    "    patches = patchify(img,(patch_size,patch_size,3), step=patch_size)\n",
    " \n",
    "    #patach 저장시키는 코드\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            single_patch = patches[i,j,0,:,:,:]\n",
    "            cv2.imwrite(save_path+'/v2_img_%s_%s.jpg'%(str(i),str(j)),single_patch)\n",
    "            p_num = (i+1)*(j+1) #patch의 개수 반환해줌 \n",
    "    return p_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e2f34-9a62-4855-b5de-9b180eb9f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_path = aug_ver2_path+'/af_pad.jpg'\n",
    "lbl_pad_path=lbl_aug_ver2_path+'/af_pad.jpg'\n",
    "\n",
    "save_patches_ver2(pad_path,aug2_patch_path,patch_size)\n",
    "save_patches_ver2(lbl_pad_path,lbl_aug2_patch_path,patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bba02-2f5c-4809-ab8d-0aa64835941c",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9eb79-7ae0-46c3-b26f-e62e32ae12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON = [a,b,\n",
    "        flip_dic['rotation range (degree)'],\n",
    "        flip_dic['rotation degree'],\n",
    "        flip_dic['vertical flip'],flip_dic['horizental flip'],\n",
    "        p_num, patch_size, alpha, shear]\n",
    "\n",
    "mkJson(JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9d74e-81d2-4320-b483-50119e974ab5",
   "metadata": {},
   "source": [
    "# Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c719b7e-6107-436a-a40f-3b7732f63ece",
   "metadata": {},
   "source": [
    "data에 사용되는 class 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac9e8f7-d8c5-4ee8-9e7a-34387bff4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel labels in the image\n",
    "\n",
    "'''\n",
    "what class_names?\n",
    "\n",
    " * bk = black color in label\n",
    " * re = red color in label\n",
    " * bl = blue color in label\n",
    "\n",
    "'''\n",
    "class_names = ['bk','re','bl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9a0fb-cb16-4b42-9acc-9df8661da552",
   "metadata": {},
   "source": [
    "training/validation dataset을 만들기 전에 경로를 지정하고, dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a810a2e8-1221-4f53-95b6-95e5ffeb4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c79924-3025-4305-a191-5970f1c9afc0",
   "metadata": {},
   "source": [
    "dataset을 위한 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1582330-3244-40ac-84ad-6395894d063a",
   "metadata": {},
   "source": [
    "# train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05728ae-b158-4f3f-8af4-6ea8f54f0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = '../../data/data/' #어그멘테이션 1,2를 거친 데이터\n",
    "train_label_path = '../../data/label/'#어그멘테이션 1,2를 거친 라벨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0ed649-0e8a-4298-a9f2-16a666f55b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n",
      "458\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(train_image_path):\n",
    "    for i,filename in enumerate(fileList):\n",
    "        if \".jpg\" in filename.lower():\n",
    "            data_list.append(os.path.join(dirName,filename))\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(train_label_path):\n",
    "    for i,filename in enumerate(fileList):\n",
    "        if \".jpg\" in filename.lower():\n",
    "            label_list.append(os.path.join(dirName,filename))\n",
    "\n",
    "data_list.sort()\n",
    "label_list.sort()\n",
    "print(len(data_list))\n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec8f567-2804-4ee5-a306-6d57bec4211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_case_arr_X = data_list\n",
    "all_case_arr_Y = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dca69df-1ad8-4a7b-8516-f6f4f66bae0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/data/img_0_0.jpg',\n",
       " '../../data/data/img_0_1.jpg',\n",
       " '../../data/data/img_0_10.jpg',\n",
       " '../../data/data/img_0_11.jpg',\n",
       " '../../data/data/img_0_12.jpg',\n",
       " '../../data/data/img_0_2.jpg',\n",
       " '../../data/data/img_0_3.jpg',\n",
       " '../../data/data/img_0_4.jpg',\n",
       " '../../data/data/img_0_5.jpg',\n",
       " '../../data/data/img_0_6.jpg',\n",
       " '../../data/data/img_0_7.jpg',\n",
       " '../../data/data/img_0_8.jpg',\n",
       " '../../data/data/img_0_9.jpg',\n",
       " '../../data/data/img_10_0.jpg',\n",
       " '../../data/data/img_10_1.jpg',\n",
       " '../../data/data/img_10_10.jpg',\n",
       " '../../data/data/img_10_11.jpg',\n",
       " '../../data/data/img_10_12.jpg',\n",
       " '../../data/data/img_10_2.jpg',\n",
       " '../../data/data/img_10_3.jpg',\n",
       " '../../data/data/img_10_4.jpg',\n",
       " '../../data/data/img_10_5.jpg',\n",
       " '../../data/data/img_10_6.jpg',\n",
       " '../../data/data/img_10_7.jpg',\n",
       " '../../data/data/img_10_8.jpg',\n",
       " '../../data/data/img_10_9.jpg',\n",
       " '../../data/data/img_11_0.jpg',\n",
       " '../../data/data/img_11_1.jpg',\n",
       " '../../data/data/img_11_10.jpg',\n",
       " '../../data/data/img_11_11.jpg',\n",
       " '../../data/data/img_11_12.jpg',\n",
       " '../../data/data/img_11_2.jpg',\n",
       " '../../data/data/img_11_3.jpg',\n",
       " '../../data/data/img_11_4.jpg',\n",
       " '../../data/data/img_11_5.jpg',\n",
       " '../../data/data/img_11_6.jpg',\n",
       " '../../data/data/img_11_7.jpg',\n",
       " '../../data/data/img_11_8.jpg',\n",
       " '../../data/data/img_11_9.jpg',\n",
       " '../../data/data/img_12_0.jpg',\n",
       " '../../data/data/img_12_1.jpg',\n",
       " '../../data/data/img_12_10.jpg',\n",
       " '../../data/data/img_12_11.jpg',\n",
       " '../../data/data/img_12_12.jpg',\n",
       " '../../data/data/img_12_2.jpg',\n",
       " '../../data/data/img_12_3.jpg',\n",
       " '../../data/data/img_12_4.jpg',\n",
       " '../../data/data/img_12_5.jpg',\n",
       " '../../data/data/img_12_6.jpg',\n",
       " '../../data/data/img_12_7.jpg',\n",
       " '../../data/data/img_12_8.jpg',\n",
       " '../../data/data/img_12_9.jpg',\n",
       " '../../data/data/img_1_0.jpg',\n",
       " '../../data/data/img_1_1.jpg',\n",
       " '../../data/data/img_1_10.jpg',\n",
       " '../../data/data/img_1_11.jpg',\n",
       " '../../data/data/img_1_12.jpg',\n",
       " '../../data/data/img_1_2.jpg',\n",
       " '../../data/data/img_1_3.jpg',\n",
       " '../../data/data/img_1_4.jpg',\n",
       " '../../data/data/img_1_5.jpg',\n",
       " '../../data/data/img_1_6.jpg',\n",
       " '../../data/data/img_1_7.jpg',\n",
       " '../../data/data/img_1_8.jpg',\n",
       " '../../data/data/img_1_9.jpg',\n",
       " '../../data/data/img_2_0.jpg',\n",
       " '../../data/data/img_2_1.jpg',\n",
       " '../../data/data/img_2_10.jpg',\n",
       " '../../data/data/img_2_11.jpg',\n",
       " '../../data/data/img_2_12.jpg',\n",
       " '../../data/data/img_2_2.jpg',\n",
       " '../../data/data/img_2_3.jpg',\n",
       " '../../data/data/img_2_4.jpg',\n",
       " '../../data/data/img_2_5.jpg',\n",
       " '../../data/data/img_2_6.jpg',\n",
       " '../../data/data/img_2_7.jpg',\n",
       " '../../data/data/img_2_8.jpg',\n",
       " '../../data/data/img_2_9.jpg',\n",
       " '../../data/data/img_3_0.jpg',\n",
       " '../../data/data/img_3_1.jpg',\n",
       " '../../data/data/img_3_10.jpg',\n",
       " '../../data/data/img_3_11.jpg',\n",
       " '../../data/data/img_3_12.jpg',\n",
       " '../../data/data/img_3_2.jpg',\n",
       " '../../data/data/img_3_3.jpg',\n",
       " '../../data/data/img_3_4.jpg',\n",
       " '../../data/data/img_3_5.jpg',\n",
       " '../../data/data/img_3_6.jpg',\n",
       " '../../data/data/img_3_7.jpg',\n",
       " '../../data/data/img_3_8.jpg',\n",
       " '../../data/data/img_3_9.jpg',\n",
       " '../../data/data/img_4_0.jpg',\n",
       " '../../data/data/img_4_1.jpg',\n",
       " '../../data/data/img_4_10.jpg',\n",
       " '../../data/data/img_4_11.jpg',\n",
       " '../../data/data/img_4_12.jpg',\n",
       " '../../data/data/img_4_2.jpg',\n",
       " '../../data/data/img_4_3.jpg',\n",
       " '../../data/data/img_4_4.jpg',\n",
       " '../../data/data/img_4_5.jpg',\n",
       " '../../data/data/img_4_6.jpg',\n",
       " '../../data/data/img_4_7.jpg',\n",
       " '../../data/data/img_4_8.jpg',\n",
       " '../../data/data/img_4_9.jpg',\n",
       " '../../data/data/img_5_0.jpg',\n",
       " '../../data/data/img_5_1.jpg',\n",
       " '../../data/data/img_5_10.jpg',\n",
       " '../../data/data/img_5_11.jpg',\n",
       " '../../data/data/img_5_12.jpg',\n",
       " '../../data/data/img_5_2.jpg',\n",
       " '../../data/data/img_5_3.jpg',\n",
       " '../../data/data/img_5_4.jpg',\n",
       " '../../data/data/img_5_5.jpg',\n",
       " '../../data/data/img_5_6.jpg',\n",
       " '../../data/data/img_5_7.jpg',\n",
       " '../../data/data/img_5_8.jpg',\n",
       " '../../data/data/img_5_9.jpg',\n",
       " '../../data/data/img_6_0.jpg',\n",
       " '../../data/data/img_6_1.jpg',\n",
       " '../../data/data/img_6_10.jpg',\n",
       " '../../data/data/img_6_11.jpg',\n",
       " '../../data/data/img_6_12.jpg',\n",
       " '../../data/data/img_6_2.jpg',\n",
       " '../../data/data/img_6_3.jpg',\n",
       " '../../data/data/img_6_4.jpg',\n",
       " '../../data/data/img_6_5.jpg',\n",
       " '../../data/data/img_6_6.jpg',\n",
       " '../../data/data/img_6_7.jpg',\n",
       " '../../data/data/img_6_8.jpg',\n",
       " '../../data/data/img_6_9.jpg',\n",
       " '../../data/data/img_7_0.jpg',\n",
       " '../../data/data/img_7_1.jpg',\n",
       " '../../data/data/img_7_10.jpg',\n",
       " '../../data/data/img_7_11.jpg',\n",
       " '../../data/data/img_7_12.jpg',\n",
       " '../../data/data/img_7_2.jpg',\n",
       " '../../data/data/img_7_3.jpg',\n",
       " '../../data/data/img_7_4.jpg',\n",
       " '../../data/data/img_7_5.jpg',\n",
       " '../../data/data/img_7_6.jpg',\n",
       " '../../data/data/img_7_7.jpg',\n",
       " '../../data/data/img_7_8.jpg',\n",
       " '../../data/data/img_7_9.jpg',\n",
       " '../../data/data/img_8_0.jpg',\n",
       " '../../data/data/img_8_1.jpg',\n",
       " '../../data/data/img_8_10.jpg',\n",
       " '../../data/data/img_8_11.jpg',\n",
       " '../../data/data/img_8_12.jpg',\n",
       " '../../data/data/img_8_2.jpg',\n",
       " '../../data/data/img_8_3.jpg',\n",
       " '../../data/data/img_8_4.jpg',\n",
       " '../../data/data/img_8_5.jpg',\n",
       " '../../data/data/img_8_6.jpg',\n",
       " '../../data/data/img_8_7.jpg',\n",
       " '../../data/data/img_8_8.jpg',\n",
       " '../../data/data/img_8_9.jpg',\n",
       " '../../data/data/img_9_0.jpg',\n",
       " '../../data/data/img_9_1.jpg',\n",
       " '../../data/data/img_9_10.jpg',\n",
       " '../../data/data/img_9_11.jpg',\n",
       " '../../data/data/img_9_12.jpg',\n",
       " '../../data/data/img_9_2.jpg',\n",
       " '../../data/data/img_9_3.jpg',\n",
       " '../../data/data/img_9_4.jpg',\n",
       " '../../data/data/img_9_5.jpg',\n",
       " '../../data/data/img_9_6.jpg',\n",
       " '../../data/data/img_9_7.jpg',\n",
       " '../../data/data/img_9_8.jpg',\n",
       " '../../data/data/img_9_9.jpg',\n",
       " '../../data/data/v2_img_0_0.jpg',\n",
       " '../../data/data/v2_img_0_1.jpg',\n",
       " '../../data/data/v2_img_0_10.jpg',\n",
       " '../../data/data/v2_img_0_11.jpg',\n",
       " '../../data/data/v2_img_0_12.jpg',\n",
       " '../../data/data/v2_img_0_13.jpg',\n",
       " '../../data/data/v2_img_0_14.jpg',\n",
       " '../../data/data/v2_img_0_15.jpg',\n",
       " '../../data/data/v2_img_0_16.jpg',\n",
       " '../../data/data/v2_img_0_2.jpg',\n",
       " '../../data/data/v2_img_0_3.jpg',\n",
       " '../../data/data/v2_img_0_4.jpg',\n",
       " '../../data/data/v2_img_0_5.jpg',\n",
       " '../../data/data/v2_img_0_6.jpg',\n",
       " '../../data/data/v2_img_0_7.jpg',\n",
       " '../../data/data/v2_img_0_8.jpg',\n",
       " '../../data/data/v2_img_0_9.jpg',\n",
       " '../../data/data/v2_img_10_0.jpg',\n",
       " '../../data/data/v2_img_10_1.jpg',\n",
       " '../../data/data/v2_img_10_10.jpg',\n",
       " '../../data/data/v2_img_10_11.jpg',\n",
       " '../../data/data/v2_img_10_12.jpg',\n",
       " '../../data/data/v2_img_10_13.jpg',\n",
       " '../../data/data/v2_img_10_14.jpg',\n",
       " '../../data/data/v2_img_10_15.jpg',\n",
       " '../../data/data/v2_img_10_16.jpg',\n",
       " '../../data/data/v2_img_10_2.jpg',\n",
       " '../../data/data/v2_img_10_3.jpg',\n",
       " '../../data/data/v2_img_10_4.jpg',\n",
       " '../../data/data/v2_img_10_5.jpg',\n",
       " '../../data/data/v2_img_10_6.jpg',\n",
       " '../../data/data/v2_img_10_7.jpg',\n",
       " '../../data/data/v2_img_10_8.jpg',\n",
       " '../../data/data/v2_img_10_9.jpg',\n",
       " '../../data/data/v2_img_11_0.jpg',\n",
       " '../../data/data/v2_img_11_1.jpg',\n",
       " '../../data/data/v2_img_11_10.jpg',\n",
       " '../../data/data/v2_img_11_11.jpg',\n",
       " '../../data/data/v2_img_11_12.jpg',\n",
       " '../../data/data/v2_img_11_13.jpg',\n",
       " '../../data/data/v2_img_11_14.jpg',\n",
       " '../../data/data/v2_img_11_15.jpg',\n",
       " '../../data/data/v2_img_11_16.jpg',\n",
       " '../../data/data/v2_img_11_2.jpg',\n",
       " '../../data/data/v2_img_11_3.jpg',\n",
       " '../../data/data/v2_img_11_4.jpg',\n",
       " '../../data/data/v2_img_11_5.jpg',\n",
       " '../../data/data/v2_img_11_6.jpg',\n",
       " '../../data/data/v2_img_11_7.jpg',\n",
       " '../../data/data/v2_img_11_8.jpg',\n",
       " '../../data/data/v2_img_11_9.jpg',\n",
       " '../../data/data/v2_img_12_0.jpg',\n",
       " '../../data/data/v2_img_12_1.jpg',\n",
       " '../../data/data/v2_img_12_10.jpg',\n",
       " '../../data/data/v2_img_12_11.jpg',\n",
       " '../../data/data/v2_img_12_12.jpg',\n",
       " '../../data/data/v2_img_12_13.jpg',\n",
       " '../../data/data/v2_img_12_14.jpg',\n",
       " '../../data/data/v2_img_12_15.jpg',\n",
       " '../../data/data/v2_img_12_16.jpg',\n",
       " '../../data/data/v2_img_12_2.jpg',\n",
       " '../../data/data/v2_img_12_3.jpg',\n",
       " '../../data/data/v2_img_12_4.jpg',\n",
       " '../../data/data/v2_img_12_5.jpg',\n",
       " '../../data/data/v2_img_12_6.jpg',\n",
       " '../../data/data/v2_img_12_7.jpg',\n",
       " '../../data/data/v2_img_12_8.jpg',\n",
       " '../../data/data/v2_img_12_9.jpg',\n",
       " '../../data/data/v2_img_13_0.jpg',\n",
       " '../../data/data/v2_img_13_1.jpg',\n",
       " '../../data/data/v2_img_13_10.jpg',\n",
       " '../../data/data/v2_img_13_11.jpg',\n",
       " '../../data/data/v2_img_13_12.jpg',\n",
       " '../../data/data/v2_img_13_13.jpg',\n",
       " '../../data/data/v2_img_13_14.jpg',\n",
       " '../../data/data/v2_img_13_15.jpg',\n",
       " '../../data/data/v2_img_13_16.jpg',\n",
       " '../../data/data/v2_img_13_2.jpg',\n",
       " '../../data/data/v2_img_13_3.jpg',\n",
       " '../../data/data/v2_img_13_4.jpg',\n",
       " '../../data/data/v2_img_13_5.jpg',\n",
       " '../../data/data/v2_img_13_6.jpg',\n",
       " '../../data/data/v2_img_13_7.jpg',\n",
       " '../../data/data/v2_img_13_8.jpg',\n",
       " '../../data/data/v2_img_13_9.jpg',\n",
       " '../../data/data/v2_img_14_0.jpg',\n",
       " '../../data/data/v2_img_14_1.jpg',\n",
       " '../../data/data/v2_img_14_10.jpg',\n",
       " '../../data/data/v2_img_14_11.jpg',\n",
       " '../../data/data/v2_img_14_12.jpg',\n",
       " '../../data/data/v2_img_14_13.jpg',\n",
       " '../../data/data/v2_img_14_14.jpg',\n",
       " '../../data/data/v2_img_14_15.jpg',\n",
       " '../../data/data/v2_img_14_16.jpg',\n",
       " '../../data/data/v2_img_14_2.jpg',\n",
       " '../../data/data/v2_img_14_3.jpg',\n",
       " '../../data/data/v2_img_14_4.jpg',\n",
       " '../../data/data/v2_img_14_5.jpg',\n",
       " '../../data/data/v2_img_14_6.jpg',\n",
       " '../../data/data/v2_img_14_7.jpg',\n",
       " '../../data/data/v2_img_14_8.jpg',\n",
       " '../../data/data/v2_img_14_9.jpg',\n",
       " '../../data/data/v2_img_15_0.jpg',\n",
       " '../../data/data/v2_img_15_1.jpg',\n",
       " '../../data/data/v2_img_15_10.jpg',\n",
       " '../../data/data/v2_img_15_11.jpg',\n",
       " '../../data/data/v2_img_15_12.jpg',\n",
       " '../../data/data/v2_img_15_13.jpg',\n",
       " '../../data/data/v2_img_15_14.jpg',\n",
       " '../../data/data/v2_img_15_15.jpg',\n",
       " '../../data/data/v2_img_15_16.jpg',\n",
       " '../../data/data/v2_img_15_2.jpg',\n",
       " '../../data/data/v2_img_15_3.jpg',\n",
       " '../../data/data/v2_img_15_4.jpg',\n",
       " '../../data/data/v2_img_15_5.jpg',\n",
       " '../../data/data/v2_img_15_6.jpg',\n",
       " '../../data/data/v2_img_15_7.jpg',\n",
       " '../../data/data/v2_img_15_8.jpg',\n",
       " '../../data/data/v2_img_15_9.jpg',\n",
       " '../../data/data/v2_img_16_0.jpg',\n",
       " '../../data/data/v2_img_16_1.jpg',\n",
       " '../../data/data/v2_img_16_10.jpg',\n",
       " '../../data/data/v2_img_16_11.jpg',\n",
       " '../../data/data/v2_img_16_12.jpg',\n",
       " '../../data/data/v2_img_16_13.jpg',\n",
       " '../../data/data/v2_img_16_14.jpg',\n",
       " '../../data/data/v2_img_16_15.jpg',\n",
       " '../../data/data/v2_img_16_16.jpg',\n",
       " '../../data/data/v2_img_16_2.jpg',\n",
       " '../../data/data/v2_img_16_3.jpg',\n",
       " '../../data/data/v2_img_16_4.jpg',\n",
       " '../../data/data/v2_img_16_5.jpg',\n",
       " '../../data/data/v2_img_16_6.jpg',\n",
       " '../../data/data/v2_img_16_7.jpg',\n",
       " '../../data/data/v2_img_16_8.jpg',\n",
       " '../../data/data/v2_img_16_9.jpg',\n",
       " '../../data/data/v2_img_1_0.jpg',\n",
       " '../../data/data/v2_img_1_1.jpg',\n",
       " '../../data/data/v2_img_1_10.jpg',\n",
       " '../../data/data/v2_img_1_11.jpg',\n",
       " '../../data/data/v2_img_1_12.jpg',\n",
       " '../../data/data/v2_img_1_13.jpg',\n",
       " '../../data/data/v2_img_1_14.jpg',\n",
       " '../../data/data/v2_img_1_15.jpg',\n",
       " '../../data/data/v2_img_1_16.jpg',\n",
       " '../../data/data/v2_img_1_2.jpg',\n",
       " '../../data/data/v2_img_1_3.jpg',\n",
       " '../../data/data/v2_img_1_4.jpg',\n",
       " '../../data/data/v2_img_1_5.jpg',\n",
       " '../../data/data/v2_img_1_6.jpg',\n",
       " '../../data/data/v2_img_1_7.jpg',\n",
       " '../../data/data/v2_img_1_8.jpg',\n",
       " '../../data/data/v2_img_1_9.jpg',\n",
       " '../../data/data/v2_img_2_0.jpg',\n",
       " '../../data/data/v2_img_2_1.jpg',\n",
       " '../../data/data/v2_img_2_10.jpg',\n",
       " '../../data/data/v2_img_2_11.jpg',\n",
       " '../../data/data/v2_img_2_12.jpg',\n",
       " '../../data/data/v2_img_2_13.jpg',\n",
       " '../../data/data/v2_img_2_14.jpg',\n",
       " '../../data/data/v2_img_2_15.jpg',\n",
       " '../../data/data/v2_img_2_16.jpg',\n",
       " '../../data/data/v2_img_2_2.jpg',\n",
       " '../../data/data/v2_img_2_3.jpg',\n",
       " '../../data/data/v2_img_2_4.jpg',\n",
       " '../../data/data/v2_img_2_5.jpg',\n",
       " '../../data/data/v2_img_2_6.jpg',\n",
       " '../../data/data/v2_img_2_7.jpg',\n",
       " '../../data/data/v2_img_2_8.jpg',\n",
       " '../../data/data/v2_img_2_9.jpg',\n",
       " '../../data/data/v2_img_3_0.jpg',\n",
       " '../../data/data/v2_img_3_1.jpg',\n",
       " '../../data/data/v2_img_3_10.jpg',\n",
       " '../../data/data/v2_img_3_11.jpg',\n",
       " '../../data/data/v2_img_3_12.jpg',\n",
       " '../../data/data/v2_img_3_13.jpg',\n",
       " '../../data/data/v2_img_3_14.jpg',\n",
       " '../../data/data/v2_img_3_15.jpg',\n",
       " '../../data/data/v2_img_3_16.jpg',\n",
       " '../../data/data/v2_img_3_2.jpg',\n",
       " '../../data/data/v2_img_3_3.jpg',\n",
       " '../../data/data/v2_img_3_4.jpg',\n",
       " '../../data/data/v2_img_3_5.jpg',\n",
       " '../../data/data/v2_img_3_6.jpg',\n",
       " '../../data/data/v2_img_3_7.jpg',\n",
       " '../../data/data/v2_img_3_8.jpg',\n",
       " '../../data/data/v2_img_3_9.jpg',\n",
       " '../../data/data/v2_img_4_0.jpg',\n",
       " '../../data/data/v2_img_4_1.jpg',\n",
       " '../../data/data/v2_img_4_10.jpg',\n",
       " '../../data/data/v2_img_4_11.jpg',\n",
       " '../../data/data/v2_img_4_12.jpg',\n",
       " '../../data/data/v2_img_4_13.jpg',\n",
       " '../../data/data/v2_img_4_14.jpg',\n",
       " '../../data/data/v2_img_4_15.jpg',\n",
       " '../../data/data/v2_img_4_16.jpg',\n",
       " '../../data/data/v2_img_4_2.jpg',\n",
       " '../../data/data/v2_img_4_3.jpg',\n",
       " '../../data/data/v2_img_4_4.jpg',\n",
       " '../../data/data/v2_img_4_5.jpg',\n",
       " '../../data/data/v2_img_4_6.jpg',\n",
       " '../../data/data/v2_img_4_7.jpg',\n",
       " '../../data/data/v2_img_4_8.jpg',\n",
       " '../../data/data/v2_img_4_9.jpg',\n",
       " '../../data/data/v2_img_5_0.jpg',\n",
       " '../../data/data/v2_img_5_1.jpg',\n",
       " '../../data/data/v2_img_5_10.jpg',\n",
       " '../../data/data/v2_img_5_11.jpg',\n",
       " '../../data/data/v2_img_5_12.jpg',\n",
       " '../../data/data/v2_img_5_13.jpg',\n",
       " '../../data/data/v2_img_5_14.jpg',\n",
       " '../../data/data/v2_img_5_15.jpg',\n",
       " '../../data/data/v2_img_5_16.jpg',\n",
       " '../../data/data/v2_img_5_2.jpg',\n",
       " '../../data/data/v2_img_5_3.jpg',\n",
       " '../../data/data/v2_img_5_4.jpg',\n",
       " '../../data/data/v2_img_5_5.jpg',\n",
       " '../../data/data/v2_img_5_6.jpg',\n",
       " '../../data/data/v2_img_5_7.jpg',\n",
       " '../../data/data/v2_img_5_8.jpg',\n",
       " '../../data/data/v2_img_5_9.jpg',\n",
       " '../../data/data/v2_img_6_0.jpg',\n",
       " '../../data/data/v2_img_6_1.jpg',\n",
       " '../../data/data/v2_img_6_10.jpg',\n",
       " '../../data/data/v2_img_6_11.jpg',\n",
       " '../../data/data/v2_img_6_12.jpg',\n",
       " '../../data/data/v2_img_6_13.jpg',\n",
       " '../../data/data/v2_img_6_14.jpg',\n",
       " '../../data/data/v2_img_6_15.jpg',\n",
       " '../../data/data/v2_img_6_16.jpg',\n",
       " '../../data/data/v2_img_6_2.jpg',\n",
       " '../../data/data/v2_img_6_3.jpg',\n",
       " '../../data/data/v2_img_6_4.jpg',\n",
       " '../../data/data/v2_img_6_5.jpg',\n",
       " '../../data/data/v2_img_6_6.jpg',\n",
       " '../../data/data/v2_img_6_7.jpg',\n",
       " '../../data/data/v2_img_6_8.jpg',\n",
       " '../../data/data/v2_img_6_9.jpg',\n",
       " '../../data/data/v2_img_7_0.jpg',\n",
       " '../../data/data/v2_img_7_1.jpg',\n",
       " '../../data/data/v2_img_7_10.jpg',\n",
       " '../../data/data/v2_img_7_11.jpg',\n",
       " '../../data/data/v2_img_7_12.jpg',\n",
       " '../../data/data/v2_img_7_13.jpg',\n",
       " '../../data/data/v2_img_7_14.jpg',\n",
       " '../../data/data/v2_img_7_15.jpg',\n",
       " '../../data/data/v2_img_7_16.jpg',\n",
       " '../../data/data/v2_img_7_2.jpg',\n",
       " '../../data/data/v2_img_7_3.jpg',\n",
       " '../../data/data/v2_img_7_4.jpg',\n",
       " '../../data/data/v2_img_7_5.jpg',\n",
       " '../../data/data/v2_img_7_6.jpg',\n",
       " '../../data/data/v2_img_7_7.jpg',\n",
       " '../../data/data/v2_img_7_8.jpg',\n",
       " '../../data/data/v2_img_7_9.jpg',\n",
       " '../../data/data/v2_img_8_0.jpg',\n",
       " '../../data/data/v2_img_8_1.jpg',\n",
       " '../../data/data/v2_img_8_10.jpg',\n",
       " '../../data/data/v2_img_8_11.jpg',\n",
       " '../../data/data/v2_img_8_12.jpg',\n",
       " '../../data/data/v2_img_8_13.jpg',\n",
       " '../../data/data/v2_img_8_14.jpg',\n",
       " '../../data/data/v2_img_8_15.jpg',\n",
       " '../../data/data/v2_img_8_16.jpg',\n",
       " '../../data/data/v2_img_8_2.jpg',\n",
       " '../../data/data/v2_img_8_3.jpg',\n",
       " '../../data/data/v2_img_8_4.jpg',\n",
       " '../../data/data/v2_img_8_5.jpg',\n",
       " '../../data/data/v2_img_8_6.jpg',\n",
       " '../../data/data/v2_img_8_7.jpg',\n",
       " '../../data/data/v2_img_8_8.jpg',\n",
       " '../../data/data/v2_img_8_9.jpg',\n",
       " '../../data/data/v2_img_9_0.jpg',\n",
       " '../../data/data/v2_img_9_1.jpg',\n",
       " '../../data/data/v2_img_9_10.jpg',\n",
       " '../../data/data/v2_img_9_11.jpg',\n",
       " '../../data/data/v2_img_9_12.jpg',\n",
       " '../../data/data/v2_img_9_13.jpg',\n",
       " '../../data/data/v2_img_9_14.jpg',\n",
       " '../../data/data/v2_img_9_15.jpg',\n",
       " '../../data/data/v2_img_9_16.jpg',\n",
       " '../../data/data/v2_img_9_2.jpg',\n",
       " '../../data/data/v2_img_9_3.jpg',\n",
       " '../../data/data/v2_img_9_4.jpg',\n",
       " '../../data/data/v2_img_9_5.jpg',\n",
       " '../../data/data/v2_img_9_6.jpg',\n",
       " '../../data/data/v2_img_9_7.jpg',\n",
       " '../../data/data/v2_img_9_8.jpg',\n",
       " '../../data/data/v2_img_9_9.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_case_arr_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095ab349-e317-48fb-8626-48141c4fa040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d21120d8-8db6-4056-8789-ccb5046cc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(\n",
    "    all_case_arr_X,all_case_arr_Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a053466-6c7b-4de4-8a26-2381aad6f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = x_train\n",
    "train_label_path = y_train\n",
    "\n",
    "test_image_path = x_test\n",
    "test_label_path = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf19d299-23d6-48a3-a482-d52f9560de09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/data/v2_img_2_6.jpg',\n",
       " '../../data/data/v2_img_1_15.jpg',\n",
       " '../../data/data/v2_img_0_12.jpg',\n",
       " '../../data/data/v2_img_13_1.jpg',\n",
       " '../../data/data/img_5_2.jpg',\n",
       " '../../data/data/v2_img_11_1.jpg',\n",
       " '../../data/data/img_8_10.jpg',\n",
       " '../../data/data/v2_img_2_1.jpg',\n",
       " '../../data/data/img_8_6.jpg',\n",
       " '../../data/data/v2_img_2_7.jpg',\n",
       " '../../data/data/v2_img_15_6.jpg',\n",
       " '../../data/data/img_3_2.jpg',\n",
       " '../../data/data/v2_img_5_12.jpg',\n",
       " '../../data/data/img_9_6.jpg',\n",
       " '../../data/data/v2_img_2_3.jpg',\n",
       " '../../data/data/v2_img_4_12.jpg',\n",
       " '../../data/data/img_2_6.jpg',\n",
       " '../../data/data/v2_img_12_4.jpg',\n",
       " '../../data/data/v2_img_7_4.jpg',\n",
       " '../../data/data/v2_img_1_6.jpg',\n",
       " '../../data/data/img_6_10.jpg',\n",
       " '../../data/data/v2_img_14_1.jpg',\n",
       " '../../data/data/v2_img_15_0.jpg',\n",
       " '../../data/data/v2_img_11_15.jpg',\n",
       " '../../data/data/v2_img_2_10.jpg',\n",
       " '../../data/data/v2_img_11_13.jpg',\n",
       " '../../data/data/v2_img_11_16.jpg',\n",
       " '../../data/data/v2_img_10_15.jpg',\n",
       " '../../data/data/v2_img_3_6.jpg',\n",
       " '../../data/data/v2_img_6_4.jpg',\n",
       " '../../data/data/img_11_7.jpg',\n",
       " '../../data/data/img_7_6.jpg',\n",
       " '../../data/data/v2_img_2_5.jpg',\n",
       " '../../data/data/v2_img_4_13.jpg',\n",
       " '../../data/data/v2_img_15_15.jpg',\n",
       " '../../data/data/v2_img_5_3.jpg',\n",
       " '../../data/data/img_1_4.jpg',\n",
       " '../../data/data/img_5_4.jpg',\n",
       " '../../data/data/v2_img_13_9.jpg',\n",
       " '../../data/data/v2_img_3_7.jpg',\n",
       " '../../data/data/img_0_3.jpg',\n",
       " '../../data/data/v2_img_9_13.jpg',\n",
       " '../../data/data/v2_img_1_4.jpg',\n",
       " '../../data/data/img_9_10.jpg',\n",
       " '../../data/data/img_8_4.jpg',\n",
       " '../../data/data/v2_img_0_16.jpg',\n",
       " '../../data/data/v2_img_0_8.jpg',\n",
       " '../../data/data/img_0_7.jpg',\n",
       " '../../data/data/v2_img_13_6.jpg',\n",
       " '../../data/data/img_4_9.jpg',\n",
       " '../../data/data/img_3_11.jpg',\n",
       " '../../data/data/img_11_9.jpg',\n",
       " '../../data/data/v2_img_15_8.jpg',\n",
       " '../../data/data/img_9_8.jpg',\n",
       " '../../data/data/v2_img_8_14.jpg',\n",
       " '../../data/data/v2_img_14_4.jpg',\n",
       " '../../data/data/v2_img_10_5.jpg',\n",
       " '../../data/data/v2_img_5_11.jpg',\n",
       " '../../data/data/v2_img_13_15.jpg',\n",
       " '../../data/data/v2_img_9_11.jpg',\n",
       " '../../data/data/v2_img_10_16.jpg',\n",
       " '../../data/data/img_3_8.jpg',\n",
       " '../../data/data/v2_img_13_5.jpg',\n",
       " '../../data/data/img_9_4.jpg',\n",
       " '../../data/data/v2_img_12_10.jpg',\n",
       " '../../data/data/img_8_12.jpg',\n",
       " '../../data/data/img_0_10.jpg',\n",
       " '../../data/data/img_4_1.jpg',\n",
       " '../../data/data/v2_img_4_4.jpg',\n",
       " '../../data/data/img_2_12.jpg',\n",
       " '../../data/data/v2_img_3_5.jpg',\n",
       " '../../data/data/img_6_3.jpg',\n",
       " '../../data/data/v2_img_2_9.jpg',\n",
       " '../../data/data/img_4_2.jpg',\n",
       " '../../data/data/img_8_0.jpg',\n",
       " '../../data/data/v2_img_3_16.jpg',\n",
       " '../../data/data/v2_img_7_1.jpg',\n",
       " '../../data/data/v2_img_16_4.jpg',\n",
       " '../../data/data/img_4_3.jpg',\n",
       " '../../data/data/v2_img_15_2.jpg',\n",
       " '../../data/data/img_2_11.jpg',\n",
       " '../../data/data/img_10_7.jpg',\n",
       " '../../data/data/img_11_8.jpg',\n",
       " '../../data/data/img_8_1.jpg',\n",
       " '../../data/data/img_6_2.jpg',\n",
       " '../../data/data/v2_img_0_7.jpg',\n",
       " '../../data/data/v2_img_10_9.jpg',\n",
       " '../../data/data/v2_img_0_6.jpg',\n",
       " '../../data/data/img_2_10.jpg',\n",
       " '../../data/data/v2_img_7_15.jpg',\n",
       " '../../data/data/v2_img_15_14.jpg',\n",
       " '../../data/data/v2_img_5_0.jpg',\n",
       " '../../data/data/v2_img_1_9.jpg',\n",
       " '../../data/data/v2_img_15_1.jpg',\n",
       " '../../data/data/v2_img_4_16.jpg',\n",
       " '../../data/data/v2_img_8_6.jpg',\n",
       " '../../data/data/img_6_5.jpg',\n",
       " '../../data/data/img_8_11.jpg',\n",
       " '../../data/data/v2_img_5_10.jpg',\n",
       " '../../data/data/v2_img_15_11.jpg',\n",
       " '../../data/data/v2_img_12_11.jpg',\n",
       " '../../data/data/v2_img_12_16.jpg',\n",
       " '../../data/data/img_3_5.jpg',\n",
       " '../../data/data/v2_img_6_15.jpg',\n",
       " '../../data/data/v2_img_11_9.jpg',\n",
       " '../../data/data/img_6_9.jpg',\n",
       " '../../data/data/v2_img_5_6.jpg',\n",
       " '../../data/data/v2_img_14_16.jpg',\n",
       " '../../data/data/v2_img_4_0.jpg',\n",
       " '../../data/data/v2_img_3_15.jpg',\n",
       " '../../data/data/v2_img_6_3.jpg',\n",
       " '../../data/data/v2_img_2_11.jpg',\n",
       " '../../data/data/v2_img_14_5.jpg',\n",
       " '../../data/data/v2_img_1_14.jpg',\n",
       " '../../data/data/img_5_5.jpg',\n",
       " '../../data/data/v2_img_0_3.jpg',\n",
       " '../../data/data/v2_img_7_2.jpg',\n",
       " '../../data/data/v2_img_1_13.jpg',\n",
       " '../../data/data/v2_img_16_6.jpg',\n",
       " '../../data/data/v2_img_9_0.jpg',\n",
       " '../../data/data/v2_img_2_15.jpg',\n",
       " '../../data/data/v2_img_12_6.jpg',\n",
       " '../../data/data/v2_img_16_11.jpg',\n",
       " '../../data/data/img_9_5.jpg',\n",
       " '../../data/data/img_7_3.jpg',\n",
       " '../../data/data/v2_img_10_4.jpg',\n",
       " '../../data/data/v2_img_14_12.jpg',\n",
       " '../../data/data/v2_img_12_5.jpg',\n",
       " '../../data/data/img_5_8.jpg',\n",
       " '../../data/data/img_6_11.jpg',\n",
       " '../../data/data/v2_img_9_8.jpg',\n",
       " '../../data/data/v2_img_12_12.jpg',\n",
       " '../../data/data/v2_img_4_7.jpg',\n",
       " '../../data/data/v2_img_8_7.jpg',\n",
       " '../../data/data/img_6_7.jpg',\n",
       " '../../data/data/v2_img_15_7.jpg',\n",
       " '../../data/data/v2_img_5_14.jpg',\n",
       " '../../data/data/img_5_11.jpg',\n",
       " '../../data/data/v2_img_3_1.jpg',\n",
       " '../../data/data/v2_img_16_2.jpg',\n",
       " '../../data/data/img_7_11.jpg',\n",
       " '../../data/data/v2_img_2_4.jpg',\n",
       " '../../data/data/img_12_2.jpg',\n",
       " '../../data/data/v2_img_7_8.jpg',\n",
       " '../../data/data/img_2_0.jpg',\n",
       " '../../data/data/v2_img_15_5.jpg',\n",
       " '../../data/data/img_3_4.jpg',\n",
       " '../../data/data/v2_img_13_13.jpg',\n",
       " '../../data/data/v2_img_10_0.jpg',\n",
       " '../../data/data/img_9_11.jpg',\n",
       " '../../data/data/img_0_9.jpg',\n",
       " '../../data/data/img_11_6.jpg',\n",
       " '../../data/data/img_11_10.jpg',\n",
       " '../../data/data/v2_img_0_1.jpg',\n",
       " '../../data/data/img_7_9.jpg',\n",
       " '../../data/data/v2_img_6_12.jpg',\n",
       " '../../data/data/v2_img_8_13.jpg',\n",
       " '../../data/data/v2_img_12_1.jpg',\n",
       " '../../data/data/img_4_12.jpg',\n",
       " '../../data/data/img_12_9.jpg',\n",
       " '../../data/data/v2_img_13_11.jpg',\n",
       " '../../data/data/v2_img_3_11.jpg',\n",
       " '../../data/data/v2_img_9_7.jpg',\n",
       " '../../data/data/v2_img_0_2.jpg',\n",
       " '../../data/data/img_12_10.jpg',\n",
       " '../../data/data/v2_img_4_1.jpg',\n",
       " '../../data/data/v2_img_11_11.jpg',\n",
       " '../../data/data/v2_img_15_4.jpg',\n",
       " '../../data/data/v2_img_14_0.jpg',\n",
       " '../../data/data/v2_img_5_15.jpg',\n",
       " '../../data/data/img_0_12.jpg',\n",
       " '../../data/data/v2_img_14_10.jpg',\n",
       " '../../data/data/v2_img_7_13.jpg',\n",
       " '../../data/data/img_4_6.jpg',\n",
       " '../../data/data/v2_img_12_14.jpg',\n",
       " '../../data/data/v2_img_6_10.jpg',\n",
       " '../../data/data/v2_img_11_3.jpg',\n",
       " '../../data/data/v2_img_0_10.jpg',\n",
       " '../../data/data/img_4_4.jpg',\n",
       " '../../data/data/v2_img_16_12.jpg',\n",
       " '../../data/data/v2_img_11_5.jpg',\n",
       " '../../data/data/img_1_6.jpg',\n",
       " '../../data/data/img_12_5.jpg',\n",
       " '../../data/data/img_11_3.jpg',\n",
       " '../../data/data/v2_img_14_6.jpg',\n",
       " '../../data/data/v2_img_2_13.jpg',\n",
       " '../../data/data/v2_img_10_7.jpg',\n",
       " '../../data/data/v2_img_7_11.jpg',\n",
       " '../../data/data/img_11_1.jpg',\n",
       " '../../data/data/v2_img_5_8.jpg',\n",
       " '../../data/data/v2_img_12_3.jpg',\n",
       " '../../data/data/v2_img_14_14.jpg',\n",
       " '../../data/data/v2_img_16_0.jpg',\n",
       " '../../data/data/img_9_3.jpg',\n",
       " '../../data/data/v2_img_6_0.jpg',\n",
       " '../../data/data/img_7_5.jpg',\n",
       " '../../data/data/img_1_7.jpg',\n",
       " '../../data/data/img_7_2.jpg',\n",
       " '../../data/data/img_6_8.jpg',\n",
       " '../../data/data/v2_img_8_5.jpg',\n",
       " '../../data/data/img_0_5.jpg',\n",
       " '../../data/data/v2_img_2_12.jpg',\n",
       " '../../data/data/v2_img_8_1.jpg',\n",
       " '../../data/data/img_1_9.jpg',\n",
       " '../../data/data/v2_img_16_5.jpg',\n",
       " '../../data/data/img_10_1.jpg',\n",
       " '../../data/data/img_9_0.jpg',\n",
       " '../../data/data/img_12_1.jpg',\n",
       " '../../data/data/v2_img_7_7.jpg',\n",
       " '../../data/data/v2_img_4_8.jpg',\n",
       " '../../data/data/v2_img_11_6.jpg',\n",
       " '../../data/data/v2_img_15_16.jpg',\n",
       " '../../data/data/v2_img_6_16.jpg',\n",
       " '../../data/data/v2_img_2_8.jpg',\n",
       " '../../data/data/v2_img_12_9.jpg',\n",
       " '../../data/data/v2_img_11_12.jpg',\n",
       " '../../data/data/v2_img_11_2.jpg',\n",
       " '../../data/data/v2_img_16_15.jpg',\n",
       " '../../data/data/v2_img_7_5.jpg',\n",
       " '../../data/data/v2_img_13_7.jpg',\n",
       " '../../data/data/v2_img_9_14.jpg',\n",
       " '../../data/data/v2_img_7_6.jpg',\n",
       " '../../data/data/v2_img_16_8.jpg',\n",
       " '../../data/data/v2_img_3_4.jpg',\n",
       " '../../data/data/v2_img_14_8.jpg',\n",
       " '../../data/data/v2_img_10_8.jpg',\n",
       " '../../data/data/img_9_2.jpg',\n",
       " '../../data/data/img_12_12.jpg',\n",
       " '../../data/data/v2_img_11_7.jpg',\n",
       " '../../data/data/v2_img_10_12.jpg',\n",
       " '../../data/data/v2_img_1_12.jpg',\n",
       " '../../data/data/v2_img_14_13.jpg',\n",
       " '../../data/data/img_5_1.jpg',\n",
       " '../../data/data/img_1_1.jpg',\n",
       " '../../data/data/img_0_1.jpg',\n",
       " '../../data/data/v2_img_6_8.jpg',\n",
       " '../../data/data/v2_img_9_1.jpg',\n",
       " '../../data/data/img_12_7.jpg',\n",
       " '../../data/data/img_3_10.jpg',\n",
       " '../../data/data/v2_img_11_10.jpg',\n",
       " '../../data/data/img_11_5.jpg',\n",
       " '../../data/data/v2_img_14_2.jpg',\n",
       " '../../data/data/v2_img_4_3.jpg',\n",
       " '../../data/data/img_4_0.jpg',\n",
       " '../../data/data/v2_img_3_0.jpg',\n",
       " '../../data/data/v2_img_8_2.jpg',\n",
       " '../../data/data/img_1_0.jpg',\n",
       " '../../data/data/v2_img_3_14.jpg',\n",
       " '../../data/data/v2_img_14_3.jpg',\n",
       " '../../data/data/v2_img_13_12.jpg',\n",
       " '../../data/data/img_10_0.jpg',\n",
       " '../../data/data/v2_img_1_3.jpg',\n",
       " '../../data/data/img_3_7.jpg',\n",
       " '../../data/data/v2_img_5_7.jpg',\n",
       " '../../data/data/v2_img_15_10.jpg',\n",
       " '../../data/data/img_9_7.jpg',\n",
       " '../../data/data/v2_img_2_14.jpg',\n",
       " '../../data/data/v2_img_9_2.jpg',\n",
       " '../../data/data/img_7_12.jpg',\n",
       " '../../data/data/v2_img_1_1.jpg',\n",
       " '../../data/data/v2_img_8_9.jpg',\n",
       " '../../data/data/v2_img_1_7.jpg',\n",
       " '../../data/data/v2_img_13_14.jpg',\n",
       " '../../data/data/img_1_10.jpg',\n",
       " '../../data/data/v2_img_4_15.jpg',\n",
       " '../../data/data/img_12_8.jpg',\n",
       " '../../data/data/v2_img_0_13.jpg',\n",
       " '../../data/data/v2_img_10_11.jpg',\n",
       " '../../data/data/v2_img_9_6.jpg',\n",
       " '../../data/data/v2_img_10_1.jpg',\n",
       " '../../data/data/v2_img_0_0.jpg',\n",
       " '../../data/data/img_1_3.jpg',\n",
       " '../../data/data/img_12_6.jpg',\n",
       " '../../data/data/v2_img_3_13.jpg',\n",
       " '../../data/data/v2_img_12_8.jpg',\n",
       " '../../data/data/v2_img_13_8.jpg',\n",
       " '../../data/data/img_10_5.jpg',\n",
       " '../../data/data/v2_img_1_16.jpg',\n",
       " '../../data/data/img_9_12.jpg',\n",
       " '../../data/data/v2_img_15_13.jpg',\n",
       " '../../data/data/v2_img_10_13.jpg',\n",
       " '../../data/data/v2_img_5_5.jpg',\n",
       " '../../data/data/v2_img_16_13.jpg',\n",
       " '../../data/data/v2_img_7_14.jpg',\n",
       " '../../data/data/v2_img_3_12.jpg',\n",
       " '../../data/data/v2_img_14_11.jpg',\n",
       " '../../data/data/v2_img_1_11.jpg',\n",
       " '../../data/data/img_8_3.jpg',\n",
       " '../../data/data/img_7_0.jpg',\n",
       " '../../data/data/img_8_5.jpg',\n",
       " '../../data/data/v2_img_4_11.jpg',\n",
       " '../../data/data/img_4_5.jpg',\n",
       " '../../data/data/v2_img_4_9.jpg',\n",
       " '../../data/data/img_3_6.jpg',\n",
       " '../../data/data/v2_img_2_16.jpg',\n",
       " '../../data/data/v2_img_11_4.jpg',\n",
       " '../../data/data/img_6_12.jpg',\n",
       " '../../data/data/v2_img_9_9.jpg',\n",
       " '../../data/data/img_10_4.jpg',\n",
       " '../../data/data/v2_img_10_10.jpg',\n",
       " '../../data/data/img_2_3.jpg',\n",
       " '../../data/data/img_5_10.jpg',\n",
       " '../../data/data/v2_img_14_9.jpg',\n",
       " '../../data/data/v2_img_3_2.jpg',\n",
       " '../../data/data/v2_img_8_4.jpg',\n",
       " '../../data/data/img_4_8.jpg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b2c55cb-a287-4e5a-99bc-ae9ec61304e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
    "\n",
    "# Convert image and mask files to tensors\n",
    "    img_raw = tf.io.read_file(t_filename)\n",
    "    anno_raw = tf.io.read_file(a_filename)\n",
    "    image = tf.image.decode_jpeg(img_raw)\n",
    "    annotation = tf.image.decode_jpeg(anno_raw)\n",
    "    \n",
    "    # Resize image and segmentation mask\n",
    "    image = tf.image.resize(image, (height, width,))\n",
    "    annotation = tf.image.resize(annotation, (height, width,))\n",
    "    image = tf.reshape(image, (height, width, 3,))\n",
    "    annotation = tf.cast(annotation, dtype=tf.int32)\n",
    "    annotation = tf.reshape(annotation, (height, width, 1,))\n",
    "    stack_list = []\n",
    "    \n",
    "    # Reshape segmentation masks\n",
    "    for c in range(len(class_names)):\n",
    "        mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
    "        stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
    "        annotation = tf.stack(stack_list, axis=2)\n",
    "    \n",
    "    # Normalize pixels in the input image\n",
    "    image = image / 127.5\n",
    "    image -= 1\n",
    "    \n",
    "    return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13319e58-722b-4f77-93cb-b4e701965435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
    "    '''\n",
    "    generates the lists of image and label map paths\n",
    "    \n",
    "    Args:\n",
    "        image_dir (string) -- path to the input images directory\n",
    "        label_map_dir (string) -- path to the label map directory\n",
    "    Returns:\n",
    "        image_paths (list of strings) -- paths to each image file\n",
    "        label_map_paths (list of strget_training_datasetings) -- paths to each label map\n",
    "    '''\n",
    "    image_paths = image_dir\n",
    "    label_map_paths = label_map_dir\n",
    "    \n",
    "    \n",
    "    return image_paths, label_map_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d87cd0-7812-4de3-a93c-4f7c3179aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(image_paths, label_map_paths):\n",
    "    '''\n",
    "    Prepares shuffled batches of the training set.\n",
    "    Args:\n",
    "    image_dir (string) -- path to the input images directory\n",
    "    label_map_dir (string) -- path to the label map directory\n",
    "    Returns:\n",
    "    tf Dataset containing the preprocessed train set\n",
    "    '''\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "    print('a')\n",
    "    training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "    print('b')\n",
    "    training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "    print('c')\n",
    "    training_dataset = training_dataset.batch(BATCH_SIZE)\n",
    "    training_dataset = training_dataset.repeat()\n",
    "    training_dataset = training_dataset.prefetch(-1)\n",
    "    \n",
    "    return training_dataset\n",
    "\n",
    "def get_validation_dataset(image_paths, label_map_paths):\n",
    "    '''\n",
    "    Prepares shuffled batches of the validation set.\n",
    "    Args:\n",
    "    image_dir (string) -- path to the input images directory\n",
    "    label_map_dir (string) -- path to the label map directory\n",
    "    Returns:\n",
    "    tf Dataset containing the preprocessed train set\n",
    "    '''\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "    validation_dataset = validation_dataset.map(map_filename_to_image_and_mask)\n",
    "    validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.repeat()\n",
    "    \n",
    "    return validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e7398e-fd01-4ac1-9c89-73b1dbed031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to the images\n",
    "training_image_paths, training_label_map_paths = train_image_path, train_label_path\n",
    "validation_image_paths, validation_label_map_paths = test_image_path, test_label_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98bf930d-4a34-4427-a0f1-865e01c16004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "# generate the train and valid sets\n",
    "  \n",
    "training_dataset = get_training_dataset(training_image_paths, training_label_map_paths)\n",
    "validation_dataset = get_validation_dataset(validation_image_paths, validation_label_map_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dc902e9-456b-4429-988f-985b22351d17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 224, 224, 3)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbde0f65-aba4-4b20-9c63-f726cd05ad4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 224, 224, 3), (None, 224, 224, 3)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4728b6-c682-4467-8dab-cb15d839c304",
   "metadata": {},
   "source": [
    "# 다음으로는 각 클래스의 segmentation 색상을 지정하도록 한다. seaborn의 color_pallette를 사용해서 RGB값을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d34fe6bb-ffcb-4857-9ac6-4aa7f23fa39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bk -- (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
      "re -- (1.0, 0.4980392156862745, 0.054901960784313725)\n",
      "bl -- (0.17254901960784313, 0.6274509803921569, 0.17254901960784313)\n"
     ]
    }
   ],
   "source": [
    "# generate a list that contains one color for each class\n",
    "colors = sns.color_palette(None, len(class_names))\n",
    "\n",
    "# print class name - normalized RGB tuple pairs\n",
    "# the tuple values will be multiplied by 255 in the helper functions later\n",
    "# to convert to the (0,0,0) to (255,255,255) RGB values you might be familiar with\n",
    "for class_name, color in zip(class_names, colors):\n",
    "    print(f'{class_name} -- {color}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fb946-a82c-40d4-b683-81c76b570150",
   "metadata": {},
   "source": [
    "# 시각화를 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30fb09e5-0547-43e4-9ff5-12cde14d49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_with_pil(images):\n",
    "    '''\n",
    "    Creates a blank image and pastes input images\n",
    "    Args:\n",
    "    images (list of numpy arrays) - numpy array representations of the images to paste\n",
    "    Returns:\n",
    "    PIL Image object containing the images\n",
    "    '''\n",
    "    widths = (image.shape[1] for image in images)\n",
    "    heights = (image.shape[0] for image in images)\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        pil_image = PIL.Image.fromarray(np.uint8(im))\n",
    "        new_im.paste(pil_image, (x_offset,0))\n",
    "        x_offset += im.shape[1]\n",
    "    return new_im\n",
    "\n",
    "def give_color_to_annotation(annotation):\n",
    "    '''\n",
    "    Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
    "    the third axis represents the color channel. The label values are multiplied by\n",
    "    255 and placed in this axis to give color to the annotation\n",
    "    Args:\n",
    "    annotation (numpy array) - label map array\n",
    "    Returns:\n",
    "    the annotation array with an additional color channel/axis\n",
    "    '''\n",
    "    seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
    "    for c in range(12):\n",
    "        segc = (annotation == c)\n",
    "        seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
    "        seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
    "        seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
    "    return seg_img\n",
    "\n",
    "def show_predictions(image, labelmaps, titles, iou_list, dice_score_list):\n",
    "    '''\n",
    "    Displays the images with the ground truth and predicted label maps\n",
    "    Args:\n",
    "    image (numpy array) -- the input image\n",
    "    labelmaps (list of arrays) -- contains the predicted and ground truth label maps\n",
    "    titles (list of strings) -- display headings for the images to be displayed\n",
    "    iou_list (list of floats) -- the IOU values for each class\n",
    "    dice_score_list (list of floats) -- the Dice Score for each vlass\n",
    "    '''\n",
    "    true_img = give_color_to_annotation(labelmaps[1])\n",
    "    pred_img = give_color_to_annotation(labelmaps[0])\n",
    "    \n",
    "    image = image + 1\n",
    "    image = image * 127.5\n",
    "    images = np.uint8([image, pred_img, true_img])\n",
    "    \n",
    "    metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
    "    metrics_by_id.sort(key=lambda tup: tup[1], reverse=True) # sorts in place\n",
    "    \n",
    "    display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
    "    display_string = \"\\n\\n\".join(display_string_list)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    for idx, im in enumerate(images):\n",
    "        plt.subplot(1, 3, idx+1)\n",
    "        if idx == 1:\n",
    "            plt.xlabel(display_string)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(titles[idx], fontsize=12)\n",
    "        plt.imshow(im)\n",
    "        \n",
    "def show_annotation_and_image(image, annotation):\n",
    "    '''\n",
    "    Displays the image and its annotation side by side\n",
    "    Args:\n",
    "        image (numpy array) -- the input image\n",
    "        annotation (numpy array) -- the label map\n",
    "    '''\n",
    "    new_ann = np.argmax(annotation, axis=2)\n",
    "    seg_img = give_color_to_annotation(new_ann)\n",
    "    \n",
    "    image = image + 1\n",
    "    image = image * 127.5\n",
    "    image = np.uint8(image)\n",
    "    images = [image, seg_img]\n",
    "    \n",
    "    images = [image, seg_img]\n",
    "    fused_img = fuse_with_pil(images)\n",
    "    plt.imshow(fused_img)\n",
    "    \n",
    "def list_show_annotation(dataset):\n",
    "    '''\n",
    "    Displays images and its annotations side by side\n",
    "    Args:\n",
    "    dataset (tf Dataset) - batch of images and annotations\n",
    "    '''\n",
    "    ds = dataset.unbatch()\n",
    "    ds = ds.shuffle(buffer_size=100)\n",
    "    \n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.title(\"Images And Annotations\")\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
    "    \n",
    "    # we set the number of image-annotation pairs to 9\n",
    "    # feel free to make this a function parameter if you want\n",
    "    for idx, (image, annotation) in enumerate(ds.take(9)):\n",
    "        plt.subplot(3, 3, idx + 1)\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        show_annotation_and_image(image.numpy(), annotation.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19757390-2c8d-4469-afd3-2fb9e96fb656",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d5534-39e4-497a-8481-821a22f19421",
   "metadata": {},
   "source": [
    "## pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb188224-0c29-4f9f-aa32-de0483d5dd4e",
   "metadata": {},
   "source": [
    "### Encoder = VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ecb3f3-1509-49b1-b565-abe2c719d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):\n",
    "    '''\n",
    "    Defines a block in the VGG block\n",
    "    Args:\n",
    "        x(tensor) -- input image\n",
    "        n_convs(int) -- number of convolution lyaers to append\n",
    "        filters(int) -- number of filters for the convolution lyaers\n",
    "        activation(string or object) -- activation to use in the convolution\n",
    "        pool_size(int) -- size of the pooling layer\n",
    "        pool_stride(int) -- stride of the pooling layer\n",
    "        block_name(string) -- name of the block\n",
    "    Returns:\n",
    "        tensor containing the max-pooled output of the convolutions\n",
    "    '''\n",
    "    for i in range(n_convs):\n",
    "        x = tf.keras.layers.Conv2D(filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            padding='same',\n",
    "            name=f'{block_name}_conv{i+1}')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=pool_size,\n",
    "            strides=pool_stride,\n",
    "            name=f'{block_name}_pool{i+1}')(x)\n",
    "   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d02aa1-e6cc-43e8-9a32-8bcc78c5d654",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-19 05:45:09--  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Resolving github.com (github.com)... 15.164.81.167\n",
      "Connecting to github.com (github.com)|15.164.81.167|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211118T204509Z&X-Amz-Expires=300&X-Amz-Signature=eea08adcf74f19c601707d63e09ab56d7761e5e19d617ce04908660de8d45751&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-11-19 05:45:09--  https://github-releases.githubusercontent.com/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211118T204509Z&X-Amz-Expires=300&X-Amz-Signature=eea08adcf74f19c601707d63e09ab56d7761e5e19d617ce04908660de8d45751&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.111.154, 185.199.108.154, 185.199.110.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.111.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58889256 (56M) [application/octet-stream]\n",
      "Saving to: ‘vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
      "\n",
      "vgg16_weights_tf_di 100%[===================>]  56.16M  11.3MB/s    in 5.0s    \n",
      "\n",
      "2021-11-19 05:45:15 (11.3 MB/s) - ‘vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [58889256/58889256]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the weights\n",
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "# assign to a variable\n",
    "vgg_weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5f65e-4923-4d0a-a236-4c28c8856225",
   "metadata": {},
   "source": [
    "### VGG-16 Network 구현\n",
    "\n",
    "입력의 shape는 (224,224,3)이며, block 함수를 통해서 VGG-16 네트워크를 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8701f40-a075-40b5-990c-b6ba59eb9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16(image_input):\n",
    "    '''\n",
    "    This function defines the VGG encoder.\n",
    "    Args:\n",
    "        image_input(tensor) -- batch of images\n",
    "    Returns:\n",
    "        tuple of tensors -- output of all encoder blocks plus the final convolution layer\n",
    "    '''\n",
    "    # create 5 blocks with increasing filters at each stage\n",
    "    x = block(image_input, n_convs=2, filters=64, kernel_size=(3,3), activation='relu',\n",
    "                pool_size=(2,2), pool_stride=(2,2),\n",
    "                block_name='block1')\n",
    "    p1 = x # (112, 112, 64)\n",
    "   \n",
    "    x = block(x, n_convs=2, filters=128, kernel_size=(3,3), activation='relu',\n",
    "                pool_size=(2,2), pool_stride=(2,2),\n",
    "                block_name='block2')\n",
    "    p2 = x # (56, 56, 128)\n",
    "    \n",
    "    x = block(x, n_convs=3, filters=256, kernel_size=(3,3), activation='relu',\n",
    "                pool_size=(2,2), pool_stride=(2,2),\n",
    "                block_name='block3')\n",
    "    p3 = x # (28, 28, 256)\n",
    "    \n",
    "    x = block(x, n_convs=3, filters=512, kernel_size=(3,3), activation='relu',\n",
    "                pool_size=(2,2), pool_stride=(2,2),\n",
    "                block_name='block4')\n",
    "    p4 = x # (14, 14, 512)\n",
    "    \n",
    "    x = block(x, n_convs=3, filters=512, kernel_size=(3,3), activation='relu',\n",
    "                pool_size=(2,2), pool_stride=(2,2),\n",
    "                block_name='block5')\n",
    "    p5 = x # (7, 7, 512)\n",
    "    \n",
    "    # create the vgg model\n",
    "    vgg = tf.keras.Model(image_input, p5)\n",
    "    \n",
    "    # load the pretrained weights downloaded\n",
    "    vgg.load_weights(vgg_weights_path)\n",
    "   \n",
    "    # number of filters for the output convolutional layers\n",
    "    n = 4096\n",
    "    \n",
    "    # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n",
    "    # we can extract more features by chaining two more convolution layers.\n",
    "    c6 = tf.keras.layers.Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\")(p5)\n",
    "    c7 = tf.keras.layers.Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"conv7\")(c6)\n",
    "    \n",
    "    # return the outputs at each stage. you will only need two of these in this particular exercise\n",
    "    # but we included it all in case you want to experiment with other types of decoders.\n",
    "    \n",
    "    # 총 5개의 block이 있으며 c7 = 마지막 출력임\n",
    "    # 마지막 출력은 1x1 convolutional layer 를 통해서 depth를 class개수로 변경해줌.\n",
    "    # skip connection을 위해서 각 block에서의 pooling layer도 return.\n",
    "\n",
    "    return (p1, p2, p3, p4, c7) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586957c6-1554-499c-bd40-dde618909f76",
   "metadata": {},
   "source": [
    "### Decoder = fcn_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84540f8d-ba51-4cbd-a5b0-6cf38b4b5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(convs, n_classes):\n",
    "\n",
    "\n",
    "    '''\n",
    "    Defines the FCN 32,16,8 decoder.\n",
    "    Args:\n",
    "    convs(tuple of tensors) -- output of the encoder network\n",
    "    n_classes(int) -- number of classes\n",
    "    Returns:\n",
    "    tensor with shape (height, width, n_classes) contating class probabilities(FCN-32, FCN-16, FCN-8)\n",
    "    '''\n",
    "    # unpack the output of the encoder\n",
    "    f1, f2, f3, f4, f5 = convs\n",
    "    \"\"\"f1 = (112, 112, 64)\n",
    "    f2 = (56, 56, 128)\n",
    "    f3 = (28, 28, 256)\n",
    "    f4 = (14, 14, 512)\n",
    "    f5 = (7, 7, 512) \"\"\"\n",
    "    # FCN-32 output\n",
    "    fcn32_o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(32,32), strides=(32, 32), use_bias=False)(f5)\n",
    "    fcn32_o = tf.keras.layers.Activation('softmax')(fcn32_o)\n",
    "    \n",
    "    # upsample the output of the encoder then crop extra pixels that were introduced\n",
    "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(4,4), strides=(2,2), use_bias=False)(f5) # (16, 16, n)\n",
    "    o = tf.keras.layers.Cropping2D(cropping=(1,1))(o) # (14, 14, n)\n",
    "    \n",
    "    # load the pool4 prediction and do a 1x1 convolution to reshape it to the same shape of 'o' above\n",
    "    o2 = f4 # (14, 14, 512)\n",
    "    o2 = tf.keras.layers.Conv2D(n_classes, (1,1), activation='relu', padding='same')(o2) # (14, 14, n)\n",
    "    \n",
    "    # add the result of the upsampling and pool4 prediction\n",
    "    o = tf.keras.layers.Add()([o, o2]) # (14, 14, n)\n",
    "    \n",
    "    # FCN-16 output\n",
    "    fcn16_o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(16,16), strides=(16,16), use_bias=False)(o)\n",
    "    fcn16_o = tf.keras.layers.Activation('softmax')(fcn16_o)\n",
    "    \n",
    "    # upsample the resulting tensor of the operation you just did\n",
    "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(4,4), strides=(2,2), use_bias=False)(o) # (30, 30, n)\n",
    "    o = tf.keras.layers.Cropping2D(cropping=(1,1))(o) # (28, 28, n)\n",
    "    \n",
    "    # load the pool3 prediction and do a 1x1 convolution to reshape it to shame shape of 'o' above\n",
    "    o2 = f3 # (28, 28, 256)\n",
    "    o2 = tf.keras.layers.Conv2D(n_classes, (1,1), activation='relu', padding='same')(o2) # (28, 28, n)\n",
    "    \n",
    "    # add the result of the upsampling and pool3 prediction\n",
    "    o = tf.keras.layers.Add()([o, o2]) # (28, 28, n)\n",
    "    \n",
    "    # upsample up to the size of the original image\n",
    "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(8,8), strides=(8,8), use_bias=False)(o) # (224, 224, n)\n",
    "    \n",
    "    # append a softmax to get the class probabilities\n",
    "    fcn8_o = tf.keras.layers.Activation('softmax')(o)\n",
    "    \n",
    "    return fcn32_o, fcn16_o, fcn8_o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad9ec5-ce32-4175-9152-72e0437ec87e",
   "metadata": {},
   "source": [
    "encoder와 decoder를 모두 연결해서 하나의 segmentation모델로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa754b-db2f-4ec0-b317-d7b6cd000842",
   "metadata": {},
   "source": [
    "# segmentation model - fcn 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c7b5647-29b5-4b23-9099-265a518e65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_model():\n",
    "    '''\n",
    "    Defines the final segmentation model by chaining together the encoder and decoder.\n",
    "   \n",
    "    Returns:\n",
    "        Keras Model that connects the encoder and decoder networks of the segmentation model\n",
    "    '''\n",
    "    inputs = tf.keras.layers.Input(shape=(224,224,3,))\n",
    "    convs = VGG_16(inputs)\n",
    "    fcn32, fcn16, fcn8 = decoder(convs, 3)\n",
    "    model_fcn32 = tf.keras.Model(inputs, fcn32)\n",
    "    model_fcn16 = tf.keras.Model(inputs, fcn16)\n",
    "    model_fcn8 = tf.keras.Model(inputs, fcn8)\n",
    "    return model_fcn32, model_fcn16, model_fcn8\n",
    "    \n",
    "    \n",
    "model_fcn32, model_fcn16, model_fcn8 = segmentation_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd944045-0c83-40c7-a8be-f277f7d58181",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae471d8-5b03-4bd6-8ddf-f8edc5b7ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_fcn32.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=sgd,\n",
    "                    metrics=['acc'])\n",
    "\n",
    "model_fcn16.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=sgd,\n",
    "                    metrics=['acc'])\n",
    "\n",
    "model_fcn8.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=sgd,\n",
    "                    metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5da10-587d-4cd7-a234-0211c665c119",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee3f0557-827b-4c72-84fd-f4933b88fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training images\n",
    "train_count = len(training_image_paths)\n",
    "# number of validation images\n",
    "valid_count = len(validation_image_paths)\n",
    "\n",
    "steps_per_epoch = train_count//BATCH_SIZE\n",
    "validation_steps = valid_count//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "361befad-0f23-4b3c-abdc-64fff81dcf23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 4s 884ms/step - loss: 2.0495 - acc: 0.3426 - val_loss: 2.0859 - val_acc: 0.3441\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 2.0546 - acc: 0.3454 - val_loss: 2.0838 - val_acc: 0.3478\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 3s 855ms/step - loss: 2.0465 - acc: 0.3500 - val_loss: 2.0807 - val_acc: 0.3535\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 3s 854ms/step - loss: 2.0311 - acc: 0.3560 - val_loss: 2.0760 - val_acc: 0.3600\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 3s 850ms/step - loss: 2.0369 - acc: 0.3616 - val_loss: 2.0688 - val_acc: 0.3636\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 2.0271 - acc: 0.3657 - val_loss: 2.0574 - val_acc: 0.3739\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 4s 881ms/step - loss: 2.0271 - acc: 0.3811 - val_loss: 2.0385 - val_acc: 0.3922\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 3s 864ms/step - loss: 1.9763 - acc: 0.3973 - val_loss: 2.0044 - val_acc: 0.4077\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 3s 866ms/step - loss: 1.9547 - acc: 0.4123 - val_loss: 1.9455 - val_acc: 0.4141\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 3s 866ms/step - loss: 1.8758 - acc: 0.4180 - val_loss: 1.8502 - val_acc: 0.4307\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 4s 881ms/step - loss: 1.7734 - acc: 0.4337 - val_loss: 1.7104 - val_acc: 0.4458\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 4s 906ms/step - loss: 1.6063 - acc: 0.4603 - val_loss: 1.5184 - val_acc: 0.4821\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 3s 864ms/step - loss: 1.4182 - acc: 0.5067 - val_loss: 1.4054 - val_acc: 0.5409\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 4s 883ms/step - loss: 1.3766 - acc: 0.5271 - val_loss: 1.3739 - val_acc: 0.5126\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.3308 - acc: 0.5117 - val_loss: 1.3516 - val_acc: 0.4918\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 4s 896ms/step - loss: 1.3235 - acc: 0.4845 - val_loss: 1.3440 - val_acc: 0.4734\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 4s 888ms/step - loss: 1.3206 - acc: 0.4851 - val_loss: 1.3395 - val_acc: 0.4982\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 4s 943ms/step - loss: 1.3169 - acc: 0.4934 - val_loss: 1.3365 - val_acc: 0.4817\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 4s 892ms/step - loss: 1.3110 - acc: 0.4800 - val_loss: 1.3345 - val_acc: 0.4789\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 3s 872ms/step - loss: 1.3058 - acc: 0.4813 - val_loss: 1.3328 - val_acc: 0.4848\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 4s 902ms/step - loss: 1.3036 - acc: 0.4830 - val_loss: 1.3314 - val_acc: 0.4795\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 4s 913ms/step - loss: 1.3112 - acc: 0.4786 - val_loss: 1.3303 - val_acc: 0.4785\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 1.3119 - acc: 0.4784 - val_loss: 1.3293 - val_acc: 0.4783\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 4s 884ms/step - loss: 1.3000 - acc: 0.4784 - val_loss: 1.3286 - val_acc: 0.4788\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 4s 883ms/step - loss: 1.3034 - acc: 0.4787 - val_loss: 1.3279 - val_acc: 0.4787\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 4s 884ms/step - loss: 1.3061 - acc: 0.4791 - val_loss: 1.3273 - val_acc: 0.4793\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 4s 917ms/step - loss: 1.2943 - acc: 0.4788 - val_loss: 1.3268 - val_acc: 0.4790\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 3s 874ms/step - loss: 1.3082 - acc: 0.4790 - val_loss: 1.3264 - val_acc: 0.4793\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 4s 888ms/step - loss: 1.3103 - acc: 0.4794 - val_loss: 1.3260 - val_acc: 0.4791\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 1.2939 - acc: 0.4793 - val_loss: 1.3256 - val_acc: 0.4790\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 4s 912ms/step - loss: 1.3037 - acc: 0.4793 - val_loss: 1.3253 - val_acc: 0.4788\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 4s 913ms/step - loss: 1.2959 - acc: 0.4793 - val_loss: 1.3250 - val_acc: 0.4798\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 3s 865ms/step - loss: 1.3076 - acc: 0.4794 - val_loss: 1.3247 - val_acc: 0.4792\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 4s 886ms/step - loss: 1.3036 - acc: 0.4790 - val_loss: 1.3245 - val_acc: 0.4791\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 4s 875ms/step - loss: 1.2932 - acc: 0.4791 - val_loss: 1.3243 - val_acc: 0.4781\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 4s 914ms/step - loss: 1.3122 - acc: 0.4788 - val_loss: 1.3241 - val_acc: 0.4784\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 4s 899ms/step - loss: 1.2934 - acc: 0.4784 - val_loss: 1.3239 - val_acc: 0.4795\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 4s 897ms/step - loss: 1.2971 - acc: 0.4789 - val_loss: 1.3237 - val_acc: 0.4784\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 4s 891ms/step - loss: 1.2972 - acc: 0.4784 - val_loss: 1.3235 - val_acc: 0.4782\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 3s 873ms/step - loss: 1.2983 - acc: 0.4784 - val_loss: 1.3234 - val_acc: 0.4783\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 4s 904ms/step - loss: 1.2975 - acc: 0.4785 - val_loss: 1.3233 - val_acc: 0.4782\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 4s 897ms/step - loss: 1.2989 - acc: 0.4784 - val_loss: 1.3231 - val_acc: 0.4787\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 3s 875ms/step - loss: 1.3092 - acc: 0.4782 - val_loss: 1.3230 - val_acc: 0.4778\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 4s 887ms/step - loss: 1.2905 - acc: 0.4781 - val_loss: 1.3229 - val_acc: 0.4775\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 4s 882ms/step - loss: 1.2994 - acc: 0.4783 - val_loss: 1.3228 - val_acc: 0.4770\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 1.3041 - acc: 0.4775 - val_loss: 1.3227 - val_acc: 0.4783\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 1.3024 - acc: 0.4777 - val_loss: 1.3226 - val_acc: 0.4777\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.2927 - acc: 0.4776 - val_loss: 1.3225 - val_acc: 0.4773\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.2939 - acc: 0.4775 - val_loss: 1.3225 - val_acc: 0.4777\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 3s 874ms/step - loss: 1.2995 - acc: 0.4782 - val_loss: 1.3224 - val_acc: 0.4787\n"
     ]
    }
   ],
   "source": [
    "history_fcn8 = model_fcn8.fit(training_dataset,\n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=validation_dataset,\n",
    "                                validation_steps=validation_steps,\n",
    "                                epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcde4743-ca74-4a72-89ba-3918b08c40a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 4s 915ms/step - loss: 2.6855 - acc: 0.3331 - val_loss: 2.1002 - val_acc: 0.3204\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 4s 898ms/step - loss: 2.0619 - acc: 0.3087 - val_loss: 2.1035 - val_acc: 0.3026\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 3s 855ms/step - loss: 2.0394 - acc: 0.2991 - val_loss: 2.1021 - val_acc: 0.3035\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 3s 844ms/step - loss: 2.0758 - acc: 0.3083 - val_loss: 2.0990 - val_acc: 0.3185\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 3s 840ms/step - loss: 2.0597 - acc: 0.3113 - val_loss: 2.0960 - val_acc: 0.3097\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 4s 878ms/step - loss: 2.0549 - acc: 0.3157 - val_loss: 2.0926 - val_acc: 0.3206\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 4s 877ms/step - loss: 2.0714 - acc: 0.3258 - val_loss: 2.0881 - val_acc: 0.3290\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 3s 843ms/step - loss: 2.0445 - acc: 0.3321 - val_loss: 2.0812 - val_acc: 0.3364\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 3s 853ms/step - loss: 2.0461 - acc: 0.3392 - val_loss: 2.0696 - val_acc: 0.3525\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 3s 853ms/step - loss: 2.0057 - acc: 0.3629 - val_loss: 2.0491 - val_acc: 0.3824\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 4s 882ms/step - loss: 1.9970 - acc: 0.3891 - val_loss: 2.0124 - val_acc: 0.3979\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 4s 889ms/step - loss: 1.9683 - acc: 0.4054 - val_loss: 1.9429 - val_acc: 0.4195\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 3s 854ms/step - loss: 1.8585 - acc: 0.4359 - val_loss: 1.8263 - val_acc: 0.4574\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 3s 853ms/step - loss: 1.7302 - acc: 0.4712 - val_loss: 1.6546 - val_acc: 0.4893\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 3s 855ms/step - loss: 1.5456 - acc: 0.4890 - val_loss: 1.4186 - val_acc: 0.4910\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 4s 883ms/step - loss: 1.3431 - acc: 0.5051 - val_loss: 1.3737 - val_acc: 0.4898\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 4s 902ms/step - loss: 1.3157 - acc: 0.5071 - val_loss: 1.3263 - val_acc: 0.4581\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 4s 887ms/step - loss: 1.3096 - acc: 0.4926 - val_loss: 1.3236 - val_acc: 0.4893\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 4s 886ms/step - loss: 1.3035 - acc: 0.5019 - val_loss: 1.3214 - val_acc: 0.4879\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 3s 872ms/step - loss: 1.2888 - acc: 0.4927 - val_loss: 1.3210 - val_acc: 0.4966\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 4s 903ms/step - loss: 1.2984 - acc: 0.4989 - val_loss: 1.3209 - val_acc: 0.5124\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 4s 932ms/step - loss: 1.2961 - acc: 0.5011 - val_loss: 1.3209 - val_acc: 0.4973\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 1.2981 - acc: 0.4984 - val_loss: 1.3209 - val_acc: 0.4983\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 3s 857ms/step - loss: 1.2973 - acc: 0.4981 - val_loss: 1.3209 - val_acc: 0.4978\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 3s 867ms/step - loss: 1.2945 - acc: 0.4981 - val_loss: 1.3209 - val_acc: 0.4983\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 4s 908ms/step - loss: 1.3006 - acc: 0.4979 - val_loss: 1.3209 - val_acc: 0.4980\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.3009 - acc: 0.4975 - val_loss: 1.3209 - val_acc: 0.4980\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 3s 868ms/step - loss: 1.2952 - acc: 0.4978 - val_loss: 1.3209 - val_acc: 0.4975\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 3s 871ms/step - loss: 1.2988 - acc: 0.4975 - val_loss: 1.3209 - val_acc: 0.4974\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 3s 869ms/step - loss: 1.2886 - acc: 0.4974 - val_loss: 1.3209 - val_acc: 0.4978\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 4s 907ms/step - loss: 1.2937 - acc: 0.4974 - val_loss: 1.3209 - val_acc: 0.4971\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 4s 894ms/step - loss: 1.3028 - acc: 0.4972 - val_loss: 1.3209 - val_acc: 0.4974\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 4s 882ms/step - loss: 1.3068 - acc: 0.4976 - val_loss: 1.3209 - val_acc: 0.4973\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 4s 875ms/step - loss: 1.2891 - acc: 0.4973 - val_loss: 1.3209 - val_acc: 0.4980\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.2922 - acc: 0.4975 - val_loss: 1.3209 - val_acc: 0.4975\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 4s 916ms/step - loss: 1.2975 - acc: 0.4973 - val_loss: 1.3209 - val_acc: 0.4986\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 4s 916ms/step - loss: 1.2992 - acc: 0.4976 - val_loss: 1.3209 - val_acc: 0.4985\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 3s 867ms/step - loss: 1.2967 - acc: 0.4978 - val_loss: 1.3209 - val_acc: 0.4980\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 1.3009 - acc: 0.4982 - val_loss: 1.3209 - val_acc: 0.4983\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 3s 872ms/step - loss: 1.2900 - acc: 0.4978 - val_loss: 1.3209 - val_acc: 0.4985\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 4s 902ms/step - loss: 1.2987 - acc: 0.4983 - val_loss: 1.3209 - val_acc: 0.4974\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 4s 910ms/step - loss: 1.2902 - acc: 0.4977 - val_loss: 1.3209 - val_acc: 0.4989\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 3s 865ms/step - loss: 1.2960 - acc: 0.4984 - val_loss: 1.3209 - val_acc: 0.4965\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 1.3073 - acc: 0.4982 - val_loss: 1.3209 - val_acc: 0.4984\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 4s 922ms/step - loss: 1.2921 - acc: 0.4984 - val_loss: 1.3209 - val_acc: 0.4969\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 4s 899ms/step - loss: 1.2912 - acc: 0.4983 - val_loss: 1.3209 - val_acc: 0.4975\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 4s 945ms/step - loss: 1.3093 - acc: 0.4984 - val_loss: 1.3209 - val_acc: 0.4997\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 4s 876ms/step - loss: 1.2849 - acc: 0.4987 - val_loss: 1.3209 - val_acc: 0.4985\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 3s 873ms/step - loss: 1.3015 - acc: 0.4986 - val_loss: 1.3209 - val_acc: 0.4972\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 3s 868ms/step - loss: 1.2978 - acc: 0.4988 - val_loss: 1.3209 - val_acc: 0.4982\n"
     ]
    }
   ],
   "source": [
    "history_fcn16 = model_fcn16.fit(training_dataset,\n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=validation_dataset,\n",
    "                                validation_steps=validation_steps,\n",
    "                                epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f05114dc-074c-47f4-ad98-1894f6a7e609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fcn16 speed:  2.6943162083625793e-06\n"
     ]
    }
   ],
   "source": [
    "print('model_fcn16 speed: ', timeit.timeit(lambda: history_fcn16, number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a75f5ff-50ec-4587-a0dd-ea56be88d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.0524 - acc: 0.3478 - val_loss: 2.0934 - val_acc: 0.3653\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 4s 909ms/step - loss: 2.0451 - acc: 0.3822 - val_loss: 2.0931 - val_acc: 0.4092\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 4s 893ms/step - loss: 2.0712 - acc: 0.4275 - val_loss: 2.0924 - val_acc: 0.4550\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 2.0563 - acc: 0.4690 - val_loss: 2.0907 - val_acc: 0.4894\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 4s 880ms/step - loss: 2.0459 - acc: 0.4953 - val_loss: 2.0860 - val_acc: 0.5029\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 4s 915ms/step - loss: 2.0529 - acc: 0.5032 - val_loss: 2.0473 - val_acc: 0.5062\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 4s 913ms/step - loss: 1.8000 - acc: 0.5039 - val_loss: 1.3856 - val_acc: 0.4935\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 2.5232 - acc: 0.4977 - val_loss: 2.0747 - val_acc: 0.4997\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 4s 886ms/step - loss: 2.0249 - acc: 0.4999 - val_loss: 2.0692 - val_acc: 0.4998\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 4s 884ms/step - loss: 2.0371 - acc: 0.4994 - val_loss: 2.0611 - val_acc: 0.4991\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 4s 906ms/step - loss: 2.0241 - acc: 0.4991 - val_loss: 2.0523 - val_acc: 0.4992\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 4s 929ms/step - loss: 2.0081 - acc: 0.4992 - val_loss: 2.0420 - val_acc: 0.4992\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 4s 890ms/step - loss: 2.0111 - acc: 0.4991 - val_loss: 2.0272 - val_acc: 0.4991\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 4s 890ms/step - loss: 1.9739 - acc: 0.4991 - val_loss: 2.0049 - val_acc: 0.4990\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 4s 911ms/step - loss: 1.9556 - acc: 0.4990 - val_loss: 1.9727 - val_acc: 0.4990\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 4s 923ms/step - loss: 1.9283 - acc: 0.4990 - val_loss: 1.9265 - val_acc: 0.4990\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 4s 916ms/step - loss: 1.8716 - acc: 0.4991 - val_loss: 1.8611 - val_acc: 0.4996\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 4s 892ms/step - loss: 1.7846 - acc: 0.4998 - val_loss: 1.7693 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 4s 906ms/step - loss: 1.7064 - acc: 0.5000 - val_loss: 1.6438 - val_acc: 0.5000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 1.5483 - acc: 0.4995 - val_loss: 1.4690 - val_acc: 0.4975\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 4s 918ms/step - loss: 1.3691 - acc: 0.4956 - val_loss: 1.3466 - val_acc: 0.4990\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 4s 920ms/step - loss: 1.3103 - acc: 0.5002 - val_loss: 1.3987 - val_acc: 0.4998\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 4s 893ms/step - loss: 1.5515 - acc: 0.4997 - val_loss: 1.5265 - val_acc: 0.4992\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 4s 918ms/step - loss: 1.5134 - acc: 0.4991 - val_loss: 1.5387 - val_acc: 0.4990\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 4s 897ms/step - loss: 1.4885 - acc: 0.4990 - val_loss: 1.4794 - val_acc: 0.4990\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 4s 973ms/step - loss: 1.4325 - acc: 0.4993 - val_loss: 1.4141 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 4s 933ms/step - loss: 1.3804 - acc: 0.5001 - val_loss: 1.3818 - val_acc: 0.5110\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 4s 911ms/step - loss: 1.3343 - acc: 0.5031 - val_loss: 1.3715 - val_acc: 0.5003\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 4s 966ms/step - loss: 1.3423 - acc: 0.5005 - val_loss: 1.3561 - val_acc: 0.5005\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 4s 911ms/step - loss: 1.3308 - acc: 0.5001 - val_loss: 1.3414 - val_acc: 0.4998\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 4s 945ms/step - loss: 1.3079 - acc: 0.4947 - val_loss: 1.3367 - val_acc: 0.5004\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 4s 932ms/step - loss: 1.3237 - acc: 0.4999 - val_loss: 1.3323 - val_acc: 0.4995\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 4s 888ms/step - loss: 1.3189 - acc: 0.4997 - val_loss: 1.3290 - val_acc: 0.5117\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 4s 905ms/step - loss: 1.2918 - acc: 0.5058 - val_loss: 1.3274 - val_acc: 0.4998\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 4s 896ms/step - loss: 1.2976 - acc: 0.4997 - val_loss: 1.3259 - val_acc: 0.4977\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 4s 925ms/step - loss: 1.3041 - acc: 0.4940 - val_loss: 1.3250 - val_acc: 0.4967\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 4s 926ms/step - loss: 1.2925 - acc: 0.4986 - val_loss: 1.3243 - val_acc: 0.5013\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 4s 931ms/step - loss: 1.3032 - acc: 0.5030 - val_loss: 1.3238 - val_acc: 0.5029\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 4s 922ms/step - loss: 1.3055 - acc: 0.5012 - val_loss: 1.3234 - val_acc: 0.4984\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 4s 918ms/step - loss: 1.2955 - acc: 0.4974 - val_loss: 1.3230 - val_acc: 0.4971\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 4s 934ms/step - loss: 1.2952 - acc: 0.4977 - val_loss: 1.3227 - val_acc: 0.4993\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 4s 969ms/step - loss: 1.3083 - acc: 0.4999 - val_loss: 1.3225 - val_acc: 0.5003\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 4s 888ms/step - loss: 1.2944 - acc: 0.5001 - val_loss: 1.3223 - val_acc: 0.4991\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 4s 895ms/step - loss: 1.2976 - acc: 0.4989 - val_loss: 1.3222 - val_acc: 0.4989\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 4s 905ms/step - loss: 1.2970 - acc: 0.4991 - val_loss: 1.3220 - val_acc: 0.4995\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 4s 955ms/step - loss: 1.2933 - acc: 0.4995 - val_loss: 1.3219 - val_acc: 0.4995\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 4s 954ms/step - loss: 1.3157 - acc: 0.4995 - val_loss: 1.3218 - val_acc: 0.4993\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 4s 915ms/step - loss: 1.2820 - acc: 0.4993 - val_loss: 1.3217 - val_acc: 0.4992\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 4s 903ms/step - loss: 1.3055 - acc: 0.4992 - val_loss: 1.3216 - val_acc: 0.4992\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 4s 898ms/step - loss: 1.2926 - acc: 0.4992 - val_loss: 1.3216 - val_acc: 0.4992\n"
     ]
    }
   ],
   "source": [
    "history_fcn32 = model_fcn32.fit(training_dataset,\n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=validation_dataset,\n",
    "                                validation_steps=validation_steps,\n",
    "                                epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db844307-aaf5-4568-984a-b4bc73898329",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8aaa3-005e-4158-87c1-d1acb0074642",
   "metadata": {},
   "source": [
    "validation dataset의 ground truth image와 label map를 우선 읽어온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c1ed27-b02d-42c1-8456-7b11e0d3dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_segments_test_arrays():\n",
    "    '''\n",
    "    Gets a subsample of the val set as your test set\n",
    "   \n",
    "    Returns:\n",
    "        Test set contatining ground truth images and label maps\n",
    "    '''\n",
    "    y_true_segments = []\n",
    "    y_true_images = []\n",
    "    test_count = 64\n",
    "    \n",
    "    ds = validation_dataset.unbatch()\n",
    "    ds = ds.batch(101)\n",
    "    \n",
    "    for image, annotation in ds.take(1):\n",
    "        y_true_images = image\n",
    "        y_true_segments = annotation\n",
    "   \n",
    "    y_true_segments = y_true_segments[:test_count, :, :, :]\n",
    "    y_true_segments = np.argmax(y_true_segments, axis=3)\n",
    "    \n",
    "    return y_true_images, y_true_segments\n",
    "    \n",
    "# load the ground truth images and segmentation masks\n",
    "y_true_images, y_true_segments = get_images_and_segments_test_arrays()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e53d5-5d04-4e34-b84b-c25ea3cfe19a",
   "metadata": {},
   "source": [
    "model의 prediction을 얻는다. 결과는 softmax output으로 각 클래스의 확률이므로, 가장 높은 확률을 인덱스만을 취한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32ed6105-e03d-47d1-b6c6-d866c0563311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "results_fcn8 = model_fcn8.predict(validation_dataset, steps=validation_steps)\n",
    "# for each pixel, get the slice number which has the highest probaility\n",
    "results_fcn8 = np.argmax(results_fcn8, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "323400ee-40ea-4223-9085-bc4070c375ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "results_fcn16 = model_fcn16.predict(validation_dataset, steps=validation_steps)\n",
    "# for each pixel, get the slice number which has the highest probaility\n",
    "results_fcn16 = np.argmax(results_fcn16, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64db3bb5-9f12-49d4-95ea-bc1cb3696fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "results_fcn32 = model_fcn32.predict(validation_dataset, steps=validation_steps)\n",
    "# for each pixel, get the slice number which has the highest probaility\n",
    "results_fcn32 = np.argmax(results_fcn32, axis=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6351e5-df87-49b9-ac98-94d7b562f646",
   "metadata": {},
   "source": [
    "## Evaluate model by IoU , Dice Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d8449-f7e2-4bc0-bbe4-55857461ef7b",
   "metadata": {},
   "source": [
    "모델을 평가하기 위해 IoU와 Dice Score를 계산하는 함수를 정의하고 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d13c4e96-6b97-444c-9dce-c1b32d1fb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Compute IoU and Dice Score\n",
    "    Args:\n",
    "        y_true(tensor) -- ground truth label map\n",
    "        y_pred(tensor) -- predicted label map\n",
    "    '''\n",
    "    class_wise_iou = []\n",
    "    class_wise_dice_score = []\n",
    "    \n",
    "    smoothening_factor = 0.00001\n",
    "\n",
    "    for i in range(12):\n",
    "        intersection = np.sum((y_pred == i) * (y_true == i))\n",
    "        y_true_area = np.sum((y_true == i))\n",
    "        y_pred_area = np.sum((y_pred == i))\n",
    "        combined_area = y_true_area + y_pred_area\n",
    "        \n",
    "        iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
    "        class_wise_iou.append(iou)\n",
    "        \n",
    "        dice_score = 2*((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
    "        class_wise_dice_score.append(dice_score)\n",
    "    return class_wise_iou, class_wise_dice_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf634b-42ab-4b99-b4e4-60af4b69b011",
   "metadata": {},
   "source": [
    "결과의 ioU와 Dice Score를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c3c536f-dafd-4d69-bb05-980f2eb828d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d26fb9c89f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# visualize the output and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults_fcn32\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"True Mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_fcn32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_score_fcn32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults_fcn16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"True Mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_fcn16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_score_fcn16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults_fcn8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_slider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"True Mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_fcn8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_score_fcn8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0d6c06f446c2>\u001b[0m in \u001b[0;36mshow_predictions\u001b[0;34m(image, labelmaps, titles, iou_list, dice_score_list)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdice_score_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mfloats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDice\u001b[0m \u001b[0mScore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mvlass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     '''\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrue_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgive_color_to_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelmaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mpred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgive_color_to_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelmaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0d6c06f446c2>\u001b[0m in \u001b[0;36mgive_color_to_annotation\u001b[0;34m(annotation)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msegc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mseg_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msegc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mseg_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msegc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mseg_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msegc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# input a number from 0 to 63 to pick an image from the test set\n",
    "integer_slider = 20\n",
    "\n",
    "# compute metrics\n",
    "iou_fcn32, dice_score_fcn32 = compute_metrics(y_true_segments[integer_slider], results_fcn32[integer_slider])\n",
    "iou_fcn16, dice_score_fcn16 = compute_metrics(y_true_segments[integer_slider], results_fcn16[integer_slider])\n",
    "iou_fcn8, dice_score_fcn8 = compute_metrics(y_true_segments[integer_slider], results_fcn8[integer_slider])\n",
    "\n",
    "# visualize the output and metrics\n",
    "show_predictions(y_true_images[integer_slider], [results_fcn32[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou_fcn32, dice_score_fcn32)\n",
    "show_predictions(y_true_images[integer_slider], [results_fcn16[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou_fcn16, dice_score_fcn16)\n",
    "show_predictions(y_true_images[integer_slider], [results_fcn8[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou_fcn8, dice_score_fcn8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0017b-7393-465c-927c-ca0190eeddc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
